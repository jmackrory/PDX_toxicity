{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Toxicity in Wikipedia Comments\n",
    "\n",
    "This is a parallel work to any work on the wikipedia toxicity data on the same\n",
    "topic.  This data has not been cleaned yet, and has not had multiple categories introduced yet.  However it is presented free from bias, for people to play with.\n",
    "\n",
    "Beware: Lots of swearing, racism, homophobia, misogyny is contained within due to nature of the comments.\n",
    "And the fact I have to search for nasty terms as a sanity check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Looking at the comments data, we'll need to clean the data quite a bit (lots of newlines, weird characters).\n",
    "\n",
    "My rough plan is to build up a lexicon, tokenize that data, and try to build a Naive Bayes model.  (Maybe later a Recurrent Neural network model?)\n",
    "\n",
    "Other Analysis possibilities:\n",
    "* Naive Bayes\n",
    "    - can find most important words in spam\n",
    "    - simple, easy to understand baseline.\n",
    "* Support Vector Machine\n",
    "    - another big architecture, less popular now?\n",
    "* Recurrent Neural Network\n",
    "    - Build up word embeddings (word2vec), or just use the pretrained ones.\n",
    "*Deep Neural Network\n",
    "    - Try to grow beyond naive Bayes via term-frequency matrix.\n",
    "* Latent Factor Analysis \n",
    "    - maybe useful prelude or alternative for building up embeddings.\n",
    "    \n",
    "Cleaning:\n",
    "* Clean data : How to remove newlines (search/replace: NEWLINE with '')\n",
    "* Tokenize (convert words to indices)\n",
    "* Stemming words\n",
    "* Balancing data set\n",
    "* Match up comments, and review scores\n",
    "* Search for gibberish words (make a new \"feature\" for badly spelled comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import log_loss,f1_score\n",
    "\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159686, 7) (1598289, 4)\n"
     ]
    }
   ],
   "source": [
    "#df_com = pd.read_csv('data/toxicity_annotated_comments_unanimous.tsv',sep='\\t')\n",
    "#df_rate = pd.read_csv('data/toxicity_annotations_unanimous.tsv',sep='\\t')\n",
    "df_com = pd.read_csv('data/toxicity_annotated_comments.tsv',sep='\\t')\n",
    "df_rate = pd.read_csv('data/toxicity_annotations.tsv',sep='\\t')\n",
    "\n",
    "#make rev_id an integer\n",
    "df_com['rev_id']=df_com['rev_id'].astype(int)\n",
    "df_rate['rev_id']=df_rate['rev_id'].astype(int)\n",
    "\n",
    "#reindex \n",
    "print(df_com.shape, df_rate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#When are the comments made?\n",
    "plt.figure()\n",
    "bin_arr=np.sort(df_com['year'].unique())\n",
    "df_com['year'].hist(bins=bin_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#make a new column in df_com with array of worker_ids, and toxicity\n",
    "df_com['scores']=None\n",
    "\n",
    "#since 'rev_id' is sorted, can take first difference, and find where\n",
    "#there are changes in 'rev_id'.  Those set the boundaries for changes.\n",
    "change_indices=df_rate.index[df_rate['rev_id'].diff()!=0].values\n",
    "\n",
    "#use numpy split to split the array into many sub-arrays.\n",
    "arr=df_rate[['worker_id','toxicity_score']].values\n",
    "split_arr=np.split(arr,change_indices)\n",
    "#drop first index as empty\n",
    "split_arr.pop(0)\n",
    "df_com['scores']=split_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def score_mean(score_list):\n",
    "    \"\"\"score_mean\n",
    "    Compute mean of toxicity scores for input array.\n",
    "    Array is first (and only) element in the input list.\n",
    "    Compute mean running down the rows.  Could be updated to include weighted sum of weights\n",
    "    \"\"\"\n",
    "    s = np.mean(score_list[:,1])\n",
    "    return s\n",
    "\n",
    "def score_median(score_list):\n",
    "    \"\"\"score_median\n",
    "    Compute median of toxicity scores for input array.\n",
    "    Array is first (and only) element in the input list.\n",
    "    Compute mean running down the rows.  Could be updated to include weighted sum of weights\n",
    "    \"\"\"\n",
    "    s = np.median(score_list[:,1])\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Make a new column computing mean, median scores\n",
    "df_com['mean_toxic']=df_com['scores'].apply(score_mean)\n",
    "df_com['median_toxic']=df_com['scores'].apply(score_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments: 159686. Toxic comments: 15362. Toxic Fraction: 0.09620129504151897\n"
     ]
    }
   ],
   "source": [
    "#Define toxic comments as those where the median is below -1, or -2.\n",
    "#-1 captures more comments, but with more variance in what is considered toxic/unhelpful.\n",
    "df_com['toxic']=(df_com['median_toxic']<=-1)\n",
    "Ntoxic=df_com['toxic'].sum()\n",
    "Ntot=len(df_com)\n",
    "print(\"Total comments: {}. Toxic comments: {}. Toxic Fraction: {}\".format(Ntot,Ntoxic,Ntoxic/Ntot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbb2592080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#When are the comments made?  Has the toxicity changed over time?\n",
    "#Note this is on the full dataset, with test/training/dev splits. \n",
    "plt.figure()\n",
    "bin_arr=np.sort(df_com['year'].unique())\n",
    "#non-toxic comments\n",
    "plt.subplot(2,2,1)\n",
    "msk1=df_com['median_toxic']<=-1\n",
    "plt.ylabel('Toxicity=-1')\n",
    "df_com['year'][msk1].hist(bins=bin_arr)\n",
    "plt.title('Toxic')\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Non-Toxic')\n",
    "df_com['year'][~msk1].hist(bins=bin_arr)\n",
    "#second row\n",
    "plt.subplot(2,2,3)\n",
    "msk2=df_com['median_toxic']<=-2\n",
    "df_com['year'][msk2].hist(bins=bin_arr)\n",
    "plt.ylabel('Toxicity=-2')\n",
    "plt.subplot(2,2,4)\n",
    "df_com['year'][~msk2].hist(bins=bin_arr)\n",
    "plt.xlabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So the data looks to be evenly balanced as toxic/non-toxic across time.\n",
    "Another question about the data is what topics were under discussion? Does this bias the output/findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#cleaning the data\n",
    "#Can use pandas built in str functionality with regex to eliminate\n",
    "#Can maybe also eliminate all punctuation?  Makes any \n",
    "\n",
    "#maybe also dates?\n",
    "def clean_up(comments):\n",
    "    com_clean=comments.str.replace('NEWLINE_TOKEN',' ')\n",
    "    com_clean=com_clean.str.replace('TAB_TOKEN',' ')    \n",
    "    #Remove HTML trash, via non-greedy replacing anything between backticks\n",
    "    com_clean=com_clean.str.replace(\"style=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"class=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"width=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"align=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"cellpadding=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"cellspacing=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"rowspan=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"colspan=\\`\\`.*?\\`\\`\",' ')\n",
    "    #remove numbers\n",
    "    com_clean=com_clean.str.replace(\"[0-9]+\",' ')\n",
    "    #remove numbers\n",
    "    com_clean=com_clean.str.replace(\"_\",' ')\n",
    "    #remove symbols.    There must be a more comprehensive way of doing this?\n",
    "    com_clean=com_clean.str.replace(\"[\\[\\[\\{\\}=_:\\|\\(\\)\\\\\\/\\`]+\",' ')\n",
    "    #remove multiple spaces, replace with a single space\n",
    "    com_clean=com_clean.str.replace('\\\\s+',' ')\n",
    "    return com_clean\n",
    "df_com['comment_clean']=clean_up(df_com['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#separate off training_split\n",
    "train_msk=df_com['split']=='train'\n",
    "df_train=df_com[train_msk]\n",
    "df_dev=df_com[df_com['split']=='dev']\n",
    "df_test=df_com[df_com['split']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95692, 125568)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#borrowing from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "count_vect=CountVectorizer(stop_words='english',lowercase=True,strip_accents='unicode')\n",
    "tfidf_vect=TfidfVectorizer(stop_words='english',lowercase=True,strip_accents='unicode')\n",
    "X_train_counts=count_vect.fit_transform(df_train['comment_clean'])\n",
    "X_train_tfidf=tfidf_vect.fit_transform(df_train['comment_clean'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#do the same transformations using existing vocab built up in training.\n",
    "X_dev_tfidf=tfidf_vect.transform(df_dev['comment_clean'])\n",
    "X_test_tfidf=tfidf_vect.transform(df_test['comment_clean'])\n",
    "\n",
    "X_dev_counts=count_vect.transform(df_dev['comment_clean'])\n",
    "X_test_counts=count_vect.transform(df_test['comment_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Checking the vectorizer and finding common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "I wanted to check that the vectorizer was working by outputting common words, and identifying the \"most toxic\" words, based on their counts.\n",
    "This was useful as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#get vocabulary dictionary\n",
    "voc_dict=count_vect.vocabulary_\n",
    "#make a dataframe, with entries as rows\n",
    "voc_df=pd.DataFrame.from_dict(voc_dict,orient='index')\n",
    "#sort by row entry value, and then use that as the index for the counts.\n",
    "voc_df1=voc_df.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29143\nName: dick, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_df1.iloc[29143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cond_prob(X_counts,toxic,csmooth=1):\n",
    "    \"\"\"cond_prob\n",
    "    Compute conditional probabilities of toxic/non-toxic words after tokenization, and \n",
    "    count vectorization. \n",
    "\n",
    "    Intput: X_counts - sparse matrix of counts of each word in a given message\n",
    "    toxic - whether word was toxic or not, with 0,1\n",
    "    csmooth - parameter for Laplace/Lidstone smoothing to account for unseen words\n",
    "\n",
    "    Return:\n",
    "    ptoxic      - total probability for toxic message\n",
    "    pword_toxic - conditional probability for word being toxic\n",
    "    pword_clean - conditional probability for word being clean\n",
    "    \"\"\"\n",
    "    nrows,nwords=X_counts.shape\n",
    "    ptoxic = np.sum(toxic)/nrows\n",
    "    \n",
    "    toxic_mat=X_counts[toxic==1,:]\n",
    "    clean_mat=X_counts[toxic==0,:]\n",
    "    #sum across messages\n",
    "    nword_toxic=np.sum(toxic_mat,axis=0)\n",
    "    nword_clean=np.sum(clean_mat,axis=0)    \n",
    "\n",
    "    #estimate probability of word given toxicity by number of times\n",
    "    #that word occurs in toxic documents, divided by the total number of words\n",
    "    #in toxic documents\n",
    "    #Laplace/Lidstone smooth version\n",
    "    pword_toxic= (nword_toxic+csmooth) \\\n",
    "                / (np.sum(toxic_mat)+nwords*csmooth)\n",
    "\n",
    "    pword_clean= (nword_clean+csmooth) \\\n",
    "                /(np.sum(clean_mat)+nwords*csmooth)\n",
    "    x1=np.sum(toxic_mat,0)\n",
    "    x2=nword_toxic\n",
    "    return ptoxic,pword_toxic,pword_clean    \n",
    "\n",
    "ptox,pw_tox,pw_cln = cond_prob( X_train_counts, df_train['toxic'].values, csmooth=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   count     pcond       p_clean   p_toxic\n",
      "fucksex            624.0  0.999987  3.640840e-09  0.002625\n",
      "buttsecks          498.0  0.999984  3.640840e-09  0.002095\n",
      "bastered           449.0  0.999982  3.640840e-09  0.001889\n",
      "cocksucker         425.0  0.999981  3.640840e-09  0.001788\n",
      "fggt               398.0  0.999980  3.640840e-09  0.001674\n",
      "mothjer            391.0  0.999979  3.640840e-09  0.001645\n",
      "offfuck            360.0  0.999978  3.640840e-09  0.001514\n",
      "niggas             340.0  0.999976  3.640840e-09  0.001430\n",
      "sexsex             332.0  0.999976  3.640840e-09  0.001396\n",
      "yourselfgo         309.0  0.999974  3.640840e-09  0.001300\n",
      "marcolfuck         260.0  0.999969  3.640840e-09  0.001094\n",
      "fack               232.0  0.999965  3.640840e-09  0.000976\n",
      "veggietales        212.0  0.999962  3.640840e-09  0.000892\n",
      "ancestryfuck       208.0  0.999961  3.640840e-09  0.000875\n",
      "notrhbysouthbanof  208.0  0.999961  3.640840e-09  0.000875\n",
      "shitfuck           182.0  0.999956  3.640840e-09  0.000766\n",
      "yaaaa              128.0  0.999937  3.640840e-09  0.000538\n",
      "haahhahahah        128.0  0.999937  3.640840e-09  0.000538\n",
      "cuntbag            128.0  0.999937  3.640840e-09  0.000538\n",
      "cuntliz            111.0  0.999927  3.640840e-09  0.000467\n",
      "pneis              105.0  0.999923  3.640840e-09  0.000442\n",
      "pennnis            105.0  0.999923  3.640840e-09  0.000442\n",
      "pensnsnniensnsn    105.0  0.999923  3.640840e-09  0.000442\n",
      "itsuck             101.0  0.999920  3.640840e-09  0.000425\n",
      "fffff               97.0  0.999917  3.640840e-09  0.000408\n",
      "uuuuuu              96.0  0.999916  3.640840e-09  0.000404\n",
      "kkkkkk              96.0  0.999916  3.640840e-09  0.000404\n",
      "gayfrozen           93.0  0.999913  3.640840e-09  0.000391\n",
      "edie                91.0  0.999911  3.640840e-09  0.000383\n",
      "cuntfranks          88.0  0.999908  3.640840e-09  0.000370\n",
      "...                  ...       ...           ...       ...\n",
      "pawlenty             1.0  0.992050  3.640840e-09  0.000004\n",
      "pees                 1.0  0.992050  3.640840e-09  0.000004\n",
      "pecie                1.0  0.992050  3.640840e-09  0.000004\n",
      "vibrators            1.0  0.992050  3.640840e-09  0.000004\n",
      "esurance             1.0  0.992050  3.640840e-09  0.000004\n",
      "parano               1.0  0.992050  3.640840e-09  0.000004\n",
      "peepee               1.0  0.992050  3.640840e-09  0.000004\n",
      "paranoic             1.0  0.992050  3.640840e-09  0.000004\n",
      "pelsa                1.0  0.992050  3.640840e-09  0.000004\n",
      "ewwwwwwwwwwwwwww     1.0  0.992050  3.640840e-09  0.000004\n",
      "paneled              1.0  0.992050  3.640840e-09  0.000004\n",
      "pelycosaur           1.0  0.992050  3.640840e-09  0.000004\n",
      "paulley              1.0  0.992050  3.640840e-09  0.000004\n",
      "peeon                1.0  0.992050  3.640840e-09  0.000004\n",
      "evlekis              1.0  0.992050  3.640840e-09  0.000004\n",
      "payal                1.0  0.992050  3.640840e-09  0.000004\n",
      "evidance             1.0  0.992050  3.640840e-09  0.000004\n",
      "vette                1.0  0.992050  3.640840e-09  0.000004\n",
      "pbass                1.0  0.992050  3.640840e-09  0.000004\n",
      "peaceniks            1.0  0.992050  3.640840e-09  0.000004\n",
      "pantaloon            1.0  0.992050  3.640840e-09  0.000004\n",
      "assclowns            1.0  0.992050  3.640840e-09  0.000004\n",
      "exaltation           1.0  0.992050  3.640840e-09  0.000004\n",
      "paragrapg            1.0  0.992050  3.640840e-09  0.000004\n",
      "vendalism            1.0  0.992050  3.640840e-09  0.000004\n",
      "asscrack             1.0  0.992050  3.640840e-09  0.000004\n",
      "esse                 1.0  0.992050  3.640840e-09  0.000004\n",
      "pel                  1.0  0.992050  3.640840e-09  0.000004\n",
      "assehole             1.0  0.992050  3.640840e-09  0.000004\n",
      "pellazg              1.0  0.992050  3.640840e-09  0.000004\n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#make new dataframe with conditional probabilities for words being toxic, and raw probabilities of occuring in toxic/clean messages\n",
    "X_cond= pw_tox*ptox/(pw_tox*ptox + pw_cln*(1-ptox))\n",
    "word_mat=np.array([X_train_counts.sum(axis=0),X_cond,pw_cln,pw_tox]).squeeze()\n",
    "word_df=pd.DataFrame(word_mat.T,columns=['count','pcond','p_clean','p_toxic'],index=voc_df1.index)\n",
    "word_df.sort_values('pcond',ascending=False,inplace=True)\n",
    "print(word_df.head(n=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So, the most toxic words (i.e. words that only appeared in toxic messages) are misspelled attempts at rudeness, with weird spaces, and combination words.  I think this reflects more on the pre-processing.  These words show up in a single toxic message, and are thus great at inferring that one message is toxic.  This doesn't say much about more general trends in the messages.\n",
    "\n",
    "I am considering also implementing a spell-check, and adding a variable for the number of incorrect words or fraction of the message that is misspelled.  Another feature would be the fraction that is capitalized?\n",
    "The accent stripping catches simple attempts to avoid the spam filter with accents, but does miss things where the words are spaced out, or have other characters inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "xtot=X_train_counts.sum(axis=0).squeeze()\n",
    "#compare vectorized vs. naive counts to check mappings\n",
    "def check_vect(count_mat,comments,vocab,word):\n",
    "    \"\"\"check_vect(count_mat,comments,vocab,word)\n",
    "    Checks the counts/occurence of words between the count vectorizer,\n",
    "    and a naive 'contains' search.  Returns all the matching comments,\n",
    "    and any discrepencies.        \n",
    "    \"\"\"\n",
    "    ind=vocab.loc[word].values\n",
    "    xtot=count_mat.sum(axis=0)\n",
    "    vect_count=(xtot[0,ind])\n",
    "    #find comments with words\n",
    "    msk=(count_mat[:,ind]>0).toarray().squeeze()\n",
    "    #find comments via naive search\n",
    "    naive_msk=comments.str.contains('{}'.format(word),case=False)\n",
    "    naive_count=np.sum(naive_msk)\n",
    "    comments=comments[msk]\n",
    "    naive_comments=comments[naive_msk]\n",
    "    diff_comments=comments[msk!=naive_msk]\n",
    "    return vect_count,naive_count,comments,naive_comments,diff_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#vc,cc,com,ncom,dcom=check_vect(X_train_counts,df_train['comment_clean'],voc_df,'gay')\n",
    "vc,cc,com,ncom,dcom=check_vect(X_dev_counts,df_dev['comment'],voc_df,'gay')\n",
    "#searching for 'fuck' gives a salutory lesson in why accent tripping is worthwhile, and a simple word filter will probably be circumvented.\n",
    "#does not account for leetspeak or rather: 13375|o3@|< (but who uses that these days?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vect: [[426]], Naive: 181\n",
      "682     NEWLINE_TOKENNEWLINE_TOKEN::Just because one is gay does not mean one is unwilling to breed. I w...\n",
      "1640    `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENI don't know who the other guy is, but of course I'm the...\n",
      "2532                       NEWLINE_TOKENNEWLINE_TOKEN== your mums ugly ==NEWLINE_TOKENNEWLINE_TOKENyour gay\n",
      "2798    `NEWLINE_TOKENNEWLINE_TOKEN== categories ==NEWLINE_TOKENNEWLINE_TOKENGood eye on those awful ``P...\n",
      "2912    `NEWLINE_TOKENNEWLINE_TOKEN== One MO' time for the kids in the back... ==NEWLINE_TOKENNEWLINE_TO...\n",
      "Name: comment, dtype: object \n",
      "\n",
      "\n",
      "682     NEWLINE_TOKENNEWLINE_TOKEN::Just because one is gay does not mean one is unwilling to breed. I w...\n",
      "1640    `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENI don't know who the other guy is, but of course I'm the...\n",
      "2532                       NEWLINE_TOKENNEWLINE_TOKEN== your mums ugly ==NEWLINE_TOKENNEWLINE_TOKENyour gay\n",
      "2798    `NEWLINE_TOKENNEWLINE_TOKEN== categories ==NEWLINE_TOKENNEWLINE_TOKENGood eye on those awful ``P...\n",
      "2912    `NEWLINE_TOKENNEWLINE_TOKEN== One MO' time for the kids in the back... ==NEWLINE_TOKENNEWLINE_TO...\n",
      "Name: comment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#currently searching for \"gay\", a term that has clean connotations, but can be used in homophobic attacks.\n",
    "#Another word with the same dichotomy of identity/hate is Jew.\n",
    "print('Vect: {}, Naive: {}'.format(vc,cc))\n",
    "print(com.head(),'\\n\\n')\n",
    "print(ncom.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck count: 6179\n",
      "fag count: 798\n",
      "kill count: 522\n",
      "bleach count: 24\n",
      "bellend count: 5\n",
      "wanker count: 1011\n",
      "towelhead not found\n"
     ]
    }
   ],
   "source": [
    "#naughty_word=['sex','fuck','shit','cunt','bitch','piss','cocksucker','dick','ass','nazi','tosser','wanker','bellend','jerk']\n",
    "#naughty_word=['fuck','fag','kill','bleach','bellend','wanker','towelhead']\n",
    "#identity_hate=['nigger','trans','faggot','kike','jew','wetback','spic']\n",
    "word_counts=X_train_counts.sum(axis=0)\n",
    "for word in naughty_word:\n",
    "    try:\n",
    "        ind=count_vect.vocabulary_[word]\n",
    "        print(word,'count: {}'.format(word_counts[0,ind]))\n",
    "    except:\n",
    "        print(word,'not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "It might be interesting to look at how the words changed over time?  Perhaps look at the prevalence of generic/homophobic/misoynistic/racist comments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "I noticed that there are very few obvious racist slurs in the unanimous data set. (lots of sexism, general hate)\n",
    "Weird sociological question on perception of toxicity of racism, perhaps by american reviewers? (this is something that the actual original project is explicitly considering at https://conversationai.github.io/bias.html)\n",
    "\n",
    "(searching for the n-word found these)\n",
    "Some ratings seem way off. e.g. the scores for comments 1467, 1657 include some -1s.\n",
    "Someone even thought 1918 was neutral!\n",
    "Wait, 2669 and 2670 are now identical comments. And some raters thought that 2670 was neutral too!  What the hell?!\n",
    "This suggests using the median toxicity score to avoid the mean being contaminated by people with a really different sense "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Naive Bayes\n",
    "\n",
    "I want to implement a Naive Bayes classifier as a baseline.  I've written my own version, which I will try to compare to\n",
    "scikit-learn's version.  (They both return the same result now).\n",
    "\n",
    "This basically treats the comments in a bag-of-words sense, and drops any correlations between the words.  Perhaps including some more\n",
    "common n-grams, e.g. \"frigging crank\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def naive_bayes(mat,pword_tox,pword_cln,ptox):\n",
    "    \"\"\"Compute probability that a message \n",
    "    is toxic via naive_bayes estimate.\n",
    "    \"\"\"\n",
    "    #I screwed up using prod_i[p(w_i|T)p(T)]\n",
    "    #instead of P(T)prod_i[p(w_i|T)].  Ugh.\n",
    "    #log probability for toxic/clean comments\n",
    "    log_Tword = np.log(pword_tox)\n",
    "    log_Cword = np.log(pword_cln)\n",
    "    ## now accumulate probabilities by multiplying number of counts\n",
    "    #per comment, with the weights per word\n",
    "    #also add on log-normalization.\n",
    "    msk=mat>0\n",
    "    log_Tscore = mat.dot(log_Tword.T)+np.log(ptox)\n",
    "    log_Cscore = mat.dot(log_Cword.T)+np.log(1-ptox)\n",
    "    #predict based on which has larger probability (or log-likelihood)\n",
    "    pred=log_Tscore>log_Cscore\n",
    "    #also output probabilities\n",
    "    prob=1/(1+np.exp(log_Cscore-log_Tscore))\n",
    "\n",
    "    return pred,prob,log_Tscore,log_Cscore,log_Tword,log_Cword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "actual=df_train['toxic'].values\n",
    "msk=actual\n",
    "Xtox = X_train_counts[msk,:]\n",
    "df_tox=df_train[msk]\n",
    "pred,prob,logT,logC,log_Tword,log_Cword=naive_bayes(X_train_counts,pw_tox,pw_cln,ptox)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb7c19cbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Plot a histogram of the log probabilities.  \n",
    "plt.figure()\n",
    "plt.hist(np.maximum(-50,np.log(prob)),bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb7541bba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a histogram of the log-odds (right term?).  \n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "bins=np.linspace(-1000,1000,100)\n",
    "plt.hist(logT-logC,bins=bins,log=True)\n",
    "plt.subplot(122)\n",
    "bins=np.linspace(-20,20,100)\n",
    "plt.hist(logT-logC,bins=bins,log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Maybe should also plot length of comments? TO what extent are these mirroring a similar underlying shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95692, 1) (95692, 1)\n",
      "True Positive 0.08485557831375663. False Positive 0.01652175730468587\n",
      "False Negative 0.011756468670317268. True Negative 0.8868661957112403\n",
      "Log-loss is 0.9767085345500724\n"
     ]
    }
   ],
   "source": [
    "def check_predictions(pred,actual,epsilon=1E-15):\n",
    "    \"\"\"check_predictions\n",
    "    Compares predicted class (y_i) against actual class (z_i).\n",
    "    Returns the confusion matrix and mean log-loss.\n",
    "    \n",
    "    Log-loss = sum_i{ z_i log[ y_i] }/M\n",
    "\n",
    "    Input: pred - predicted values (0,1)\n",
    "    actual - true labels \n",
    "    eps    - shift to avoid log(0)\n",
    "    Returns: Confusion matrix with [[true positive, false positive],[false negative, true negative]]\n",
    "    log-loss - average log-loss\n",
    "    \"\"\"\n",
    "    actual=np.reshape(actual,(len(actual),1))\n",
    "    pred=np.reshape(pred,(len(actual),1))    \n",
    "    print(pred.shape,actual.shape)\n",
    "    tp = np.mean((pred==True)&(actual==True))\n",
    "    tn = np.mean((pred==False)&(actual==False))\n",
    "    fp = np.mean((pred==True)&(actual==False))    \n",
    "    fn = np.mean((pred==False)&(actual==True))            \n",
    "    scores=np.matrix([[tp,fp],[fn,tn]])\n",
    "    print(\"True Positive {}. False Positive {}\".format(tp,fp))\n",
    "    print(\"False Negative {}. True Negative {}\".format(fn,tn))\n",
    "    pred_num=pred.astype(float)\n",
    "    logloss=log_loss(actual,pred_num,eps=epsilon,normalize=True)    \n",
    "    #give zero a small correction.\n",
    "    #pred_num[pred==False]=epsilon\n",
    "    #pred_num[pred==True]=1-epsilon\n",
    "    #my (initial) wrong attempt\n",
    "    #logloss2=-np.mean(np.multiply(actual,np.log(pred_num)))\n",
    "    # logloss2=-np.mean(np.multiply(actual,np.log(pred_num))\\\n",
    "    #     +np.multiply(1-actual,np.log(1-pred_num)))\n",
    "    # print(logloss2)\n",
    "\n",
    "    #logloss=0\n",
    "    print(\"Log-loss is {}\".format(logloss))\n",
    "    return scores,logloss\n",
    "logloss,score_rates=check_predictions(pred,actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "?log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Interesting. The mean log-loss is surprisingly sensitive to the chosen zero-offset.  I think this reflects the fact that the naive-bayes method is returning a lot of incredibly small probabilities (10^{-100})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Look at the false negatives \n",
    "# df_fn=df_train[(pred==False)]\n",
    "# df_fn=df_fn[df_fn['toxic']==True]\n",
    "# df_fn[['comment_clean','mean_toxic','median_toxic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After fixing this, I get a 50% false positive rate.  And have a 10% false negative rate.  Note that this is searching for the most toxic comments.\n",
    "\n",
    "The false negatives in that larger seem to be more rules-lawyering, whinging about admnistration, and sidestepping filters. e.g. f:)u:)c:)k:).\n",
    "This is a bit harder for the classifier to find.\n",
    "\n",
    "Also length?  Can try a SVM, and then some dimensionality reduction word2vec, then neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB(alpha=0.01)\n",
    "nb.fit(X_train_counts,df_train['toxic'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95692, 1) (95692, 1)\n",
      "True Positive 0.08485557831375663. False Positive 0.01652175730468587\n",
      "False Negative 0.011756468670317268. True Negative 0.8868661957112403\n",
      "0.651130214594\n",
      "Log-loss is 0.9767085345500724\n"
     ]
    }
   ],
   "source": [
    "pred_nb=nb.predict(X_train_counts)\n",
    "nb_stats=check_predictions(pred_nb,actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Well, I must have an error in my Naive Bayes code.  I'm getting much worse results.  Have correct class probabilities, and conditional probabilities.  (Found the bug.  Used $\\prod_i[p(w_i|T)p(T)]$ instead of\n",
    "$p(T)\\prod_i p(w_i|T)$.  Found my error after examining hidden attributes, but really checking and copying from the sklearn source code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Naive Bayes False Positives and Negatives\n",
    "\n",
    "Let's now look a bit at the misclassified results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#fixing shapes to avoid broadcasting\n",
    "actual=np.reshape(actual,(len(actual),1))\n",
    "pred=np.reshape(pred,(len(actual),1))    \n",
    "\n",
    "fp_msk = ((pred==True)&(actual==False))    \n",
    "fn_msk = ((pred==False)&(actual==True))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_fn=df_train[fn_msk][['comment_clean','mean_toxic','median_toxic','toxic']]\n",
    "df_fp=df_train[fp_msk][['comment_clean','mean_toxic','median_toxic','toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                            comment_clean  \\\n",
       "1659   == SHUT UP, FAG == LOL YOU WILL BAN ME HUH? OOH IM SHAKIN IM SHAKIN FUCKING FAGGOT YOU NEED TO ...   \n",
       "2570                                                                                DAMN ur an ugly fukka   \n",
       "2670      == Chanting Cox == Shut up you dumb cunt. Your a nigger loving, cum cuzzling crack whore bitch!   \n",
       "2773                                                              ==Suck COck!!!== you do!!!! you do!!!!    \n",
       "2880        == HOLA FUCKER RETARD WHO HAS NOTHING BETTER TO DO THAN DO WIKIPEDIA SHIT == FUCK YOU ASSHOLE   \n",
       "\n",
       "      mean_toxic  median_toxic  toxic  \n",
       "1659        -1.3          -1.0  False  \n",
       "2570        -1.1          -1.0  False  \n",
       "2670        -1.1          -1.0  False  \n",
       "2773        -1.4          -1.0  False  \n",
       "2880        -1.4          -1.5  False  "
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                             comment_clean  \\\n",
       "2464   BUSH US EVIL, HE'S SELFISH AND VERY SHORT TEMPERED. HE SHOULDN'T EVEN BE IN CONTROL OF A PET SHO...   \n",
       "3560    == GAY SEX, ANAL, ALL NIGHT == Why are you reverting me? SPUI saya on his talk page that he doe...   \n",
       "5066    No thanks faggot, I'm fine right here. But you might try going to hell. You don't like to suck ...   \n",
       "10064  ` == WHAT IN THE FUCK ARE YOU REFERRING TOO == I HAVE NO IDEA WHAT THE PISS YOU'RE REFERRING TOO...   \n",
       "11732                             == ASSHOLE! == I just read this article, and man, was Hitler an asshole!   \n",
       "\n",
       "       mean_toxic  median_toxic  toxic  \n",
       "2464         -1.4          -2.0   True  \n",
       "3560         -1.6          -2.0   True  \n",
       "5066         -1.3          -2.0   True  \n",
       "10064        -1.7          -2.0   True  \n",
       "11732        -1.6          -2.0   True  "
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ind=df_fn.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So false negatives.  Much more spacing/characters being used to avoid the filter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So at least the \"false positives\" are because the people using the rating scale are wildly inconsistent.  These are \"-1\" on the toxicity scale, and so \"non-toxic\" under the rule where toxic comments have median toxicity less than -1.\n",
    "I think I recognize some Full Metal Jacket quotes in there being used as insults.\n",
    "Some are \"neutral\" but have lots of repitition.  I can't for the life of me imagine any of these comments adding anything to the discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Let's use the truncated SVD for dimensionality reduction (or latent semantic analysis?)\n",
    "Apparently TF-IDF matrix is superior to straight term frequency matrix for this purpose  (more closely matches assumptions in the SVD about the noise.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=10,\n       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#took a minute or two\n",
    "TSVD=TruncatedSVD(n_components=100,n_iter=10)\n",
    "TSVD.fit(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#actually transform the results \n",
    "X_train_trans=TSVD.transform(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efba92d0f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(TSVD.explained_variance_)\n",
    "plt.xlabel('Singular value label')\n",
    "plt.ylabel('Singular value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We will next use the transformed results in a \"deep\" neural network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#actually transform the dev/test data.\n",
    "X_dev_trans=TSVD.transform(X_dev_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "In CS229, Andrew Ng's assignmmnt 2 suggest the SVM as a natural improvement over the Naive Bayes method.\n",
    "Let's implement one of those.  I'm going to update it to do batch gradient descent with sparse matrices.\n",
    "The version I wrote initially was trash, I am attempting to vectorize the code using appropriate scipy.sparse matrix operations.\n",
    "\n",
    "Or I could use an ensemble of SVM's based on subsets of the data. That leverages the existing (presumably smarter) scikit-learn code, in a way that could scale up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "947"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_nb.sum()\n",
    "actual=df_train['toxic'].values\n",
    "actual.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#define a cost function, check that we're minimizing it.\n",
    "#define the alternative cost function to be sure we'e also minimize that original choice.\n",
    "#check constraints are obeyed?\n",
    "def svm_cost(alpha,Kmat,cat,l):\n",
    "    m = Kmat.shape[0]\n",
    "    Ka = np.dot(Kmat,alpha);            \n",
    "    cost=0.5*l*np.dot( alpha, Ka)\n",
    "    yvec=(1-cat*Ka)/m\n",
    "    ymsk=yvec>0\n",
    "    cost+=np.sum(yvec*ymsk)\n",
    "    return cost                    \n",
    "\n",
    "#Compute Kernel Matrix, an m x m matrix\n",
    "#that effectively measures similarity between inputs.\n",
    "#each K_{ij} is the \"distance\" between the weighted inputs,\n",
    "# $x^{(i)}_k, $x^{(j)}_k$.\n",
    "# def kernel_matrix(x,tau):\n",
    "#     m=x.shape[0]\n",
    "#     x=(x>0).astype(int)\n",
    "#     K=np.zeros([m,m])\n",
    "#     for i in range(0,m):\n",
    "#         K[:,i]=np.exp(-np.sum((x-x[i])**2,1)/(2*tau**2));\n",
    "#     return K\n",
    "\n",
    "#Compute a column from the Kernel matrix.\n",
    "#Matrix is assumed to be [m,m], with vec\n",
    "#of length m.  Returns vector of length m.\n",
    "# def Kvec(mat,vec,tau):\n",
    "#     xarg=np.sum((mat-vec)**2,1)\n",
    "#     Kv=np.exp(-xarg/(2*tau**2));\n",
    "#     return Kv\n",
    "\n",
    "def Kbatch(mat,ind,norm2,tau):\n",
    "    \"\"\"Kbatch(mat,cvec,ind,norm2,tau)\n",
    "    Compute a batch of kernel matrix elements. \n",
    "    Input: mat  - sparse matrix (nobs x nfeature)\n",
    "           ind - indices for that subset of rows (nbatch)\n",
    "           norm2 - column matrix with squared norm for each (nobs,1)\n",
    "    Return: Kvecs - nbatch x nobs subset of the full kernel matrix.\n",
    "    \"\"\"\n",
    "    nbatch=len(ind)\n",
    "    #extract chosen rows\n",
    "    cvec = mat[ind,:].T\n",
    "    #relying on numpy broadcasting to join (nobs,nbatch) + (nobs,1)\n",
    "    xarg=-2.0*mat.dot(cvec)+norm2\n",
    "    #further broadcasting: use a row-vector ind to make a row-vector\n",
    "    #of relevant norms.\n",
    "    #then broadcast again from (nobs,nbatch)+ (1,nbatch)\n",
    "    xarg+=norm2[ind].T\n",
    "    Kv=np.exp(-xarg/(2*tau**2));\n",
    "    return Kv\n",
    "\n",
    "#carry out update on parameters for given loss function for SVM,\n",
    "#given parameters, a row-vector of inputs K_i\n",
    "def svm_batchsgd_update(alpha,Kbatch,y,ind,rate,l):\n",
    "    \"\"\"svm_batchsgd_update\n",
    "    alpha  - nobs x 1 vector\n",
    "    Kbatch - nobs x Nbatch subset of Kernel matrix\n",
    "    y      - (1xNbatch) labels for inputs\n",
    "    ind    - (1xNbatch) indices for batch\n",
    "    \"\"\"\n",
    "    nobs = Kbatch.shape[0]\n",
    "    yK = np.multiply(Kbatch,y.T)   #nobs x Nbatch \n",
    "    yKa = np.dot(alpha.T,yK);\n",
    "    Kalpha = np.multiply(Kbatch,alpha[ind].T)\n",
    "    #da= (-y_i*K_i)*((1-y_i*Ka) >0)+m*l*K_i*alpha[ind];\n",
    "    da= -np.multiply(yK,yKa<-1)+nobs*l*Kalpha;\n",
    "    #sum all changes over columns\n",
    "    alpha=alpha-rate*np.sum(da,axis=1);\n",
    "    return alpha\n",
    "    \n",
    "#Fit SVM coefficients for spam with stochastic gradient descent.\n",
    "#use known categories in cat_vec, and word_matrix with nobs x nwords\n",
    "def svm_fit(word_mat,cat_vec,tau=8,Nbatch=100):\n",
    "    #just count whether word occurs.\n",
    "    new_mat=(word_mat>0).astype(int)\n",
    "    nobs,nword=new_mat.shape;\n",
    "    alpha=0.1*np.random.randn(nobs,1)    #initialize parameters\n",
    "    alpha0=alpha\n",
    "    alpha_tot=np.zeros((nobs,1))\n",
    "    niter=int(40*nobs/Nbatch);\n",
    "    l=1/(tau**2*nobs)\n",
    "    norm2=new_mat.multiply(new_mat).sum(axis=1)\n",
    "    #multiple iterations of stochastic gradient descent.\n",
    "    for t in range(0,niter):\n",
    "        indx=np.random.randint(low=0,high=nobs,size=Nbatch)        \n",
    "        Kv = Kbatch(new_mat,indx,norm2,tau)\n",
    "        yt=cat_vec[indx]\n",
    "        rate=np.sqrt(np.sqrt(1.0/(t+1)))\n",
    "        alpha=svm_batchsgd_update(alpha,Kv,yt,indx,rate,l)\n",
    "        alpha_tot=alpha_tot+alpha\n",
    "        if (10*t % niter ==0):\n",
    "            print(\"Iter {} of {}\".format(t,niter))\n",
    "    alpha_tot=alpha_tot/niter\n",
    "    return alpha0,alpha_tot\n",
    "\n",
    "#given parameters, predict the output\n",
    "def svm_predict(train_mat,test_mat,alpha,tau):\n",
    "    ntrain=train_mat.shape[0]\n",
    "    ntest=test_mat.shape[0]\n",
    "    pred_cat=np.zeros(ntest)\n",
    "    train_new=(train_mat>0).astype(int)\n",
    "    test_new=(test_mat>0).astype(int)\n",
    "\n",
    "    train_norm=train_new.multiply(train_new).sum(axis=1)\n",
    "    test_norm=test_new.multiply(test_new).sum(axis=1)\n",
    "    train_test_dot = np.dot(train_new,test_new.T)\n",
    "    for i in range(0,ntest):\n",
    "        #compute dot-product of param-vector and column of kernel matrix\n",
    "        dist2 = train_norm-2*train_test_dot[:,i]+test_norm[i]\n",
    "        Kvec=np.exp(-dist2/(2*tau**2))\n",
    "        #Kvec=np.exp(-np.sum((train_new-test_new[i])**2,1)/(2*tau**2))\n",
    "        pred_size= np.dot(alpha.T,Kvec)\n",
    "        pred_cat[i] = np.sign(pred_size)\n",
    "    return pred_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "norm2=X_train_counts.multiply(X_train_counts).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Nsub=1000\n",
    "np.random.seed(454)\n",
    "def get_subset(frac_perc,dat_mat,labels):\n",
    "    \"\"\"get_subset\n",
    "    Returns random subset of the data and labels.\n",
    "    Maintains same fraction of toxic/non-toxic data as the full dataset.\n",
    "    \"\"\" \n",
    "    #make vector and sample indices for true/false.\n",
    "    nvec=np.arange(len(labels))\n",
    "    #get the indices for true/false\n",
    "    Tvec=nvec[labels]\n",
    "    Cvec=nvec[~labels]\n",
    "    #grab a random shuffling of those indices.\n",
    "    np.random.shuffle(Tvec)\n",
    "    np.random.shuffle(Cvec)\n",
    "    #grab some fraction of them.\n",
    "    it = int(len(Tvec)*frac_perc)\n",
    "    ic = int(len(Cvec)*frac_perc)\n",
    "    ind_sub=np.append(Tvec[:it],Cvec[:ic])\n",
    "    Xsub = dat_mat[ind_sub]\n",
    "    label_sub = labels[ind_sub].reshape((len(ind_sub),1))\n",
    "    return ind_sub,Xsub,label_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "ind_sub,Xsub,label_sub=get_subset(0.01,X_train_counts,actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 191 of 382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 of 382\n"
     ]
    }
   ],
   "source": [
    "#my code: super slow.\n",
    "#TODO: Look into Cython.  Does it play nice with sparse?\n",
    "alpha0,alpha=svm_fit(Xsub,label_sub,Nbatch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "svm_pred=svm_predict(Xsub,Xsub,alpha,8)\n",
    "check_predictions(svm_pred,label_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Lets try to use sklearns version on the same subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "nfeature,nobs=X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Try to determine parameters gamma/C via cross-validation.\n",
    "#Note that there is no need for explicit regularization?  Apparently in large dimensions, the parameters C/gamma (for penalty radius and width of basis function do a decent job in regularizing), since l1, l2 regularization don't work.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Since apparently the training time for a SVM goes as $O(n_{sample}^3)$, maybe it is better to train an ensemble of SVMs.\n",
    "In which case the training time is $O(n_{sample}^3/n_{ensemble^2})$ for the ensemble.  Then evaluating the results typically takes $O(n_sample)$ for all of the ensemble together.  (This is something like making the assumption that the kernels are block-diagonal, once appropriately sorted).  If we repeat this for multiple such random splits we can extract different correlations.\n",
    "Then take a majority vote.\n",
    "\n",
    "A similar idea is available here:(https://stackoverflow.com/questions/31681373/making-svm-run-faster-in-python), which suggests\n",
    "using a BaggingClassifier to automate the process.  \n",
    "Of course, Random Forests are another option, with a similar goal.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "?SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1912, 1) (1912, 1)\n",
      "True Positive 0.07426778242677824. False Positive 0.29707112970711297\n",
      "False Negative 0.021966527196652718. True Negative 0.606694560669456\n",
      "11.7227196161\n",
      "Log-loss is 11.019407830667294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1912, 1) (1912, 1)\n",
      "True Positive 0.09623430962343096. False Positive 0.2688284518828452\n",
      "False Negative 0.0. True Negative 0.6349372384937239\n",
      "9.87589722428\n",
      "Log-loss is 9.285220742710885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "frac_perc=0.02\n",
    "svm=SVC(cache_size=1000,verbose=True,gamma=0.01,C=0.5,class_weight='balanced')\n",
    "indsub,Xsub,label_sub=get_subset(frac_perc,X_train_counts,df_train['toxic'].values)\n",
    "svm.fit(Xsub,label_sub.ravel())\n",
    "svm_pred=svm.predict(Xsub)\n",
    "svm_stats=check_predictions(svm_pred,label_sub)\n",
    "#test on a different subset of the training data\n",
    "indsub2,Xsub2,label_sub2=get_subset(frac_perc,X_train_counts,df_train['toxic'].values)\n",
    "svm_pred2=svm.predict(Xsub2)\n",
    "svm_stats2=check_predictions(svm_pred2,label_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "So let's try the current flavour of the month approach: a recurrent neural network.\n",
    "Based on talking to Joseph and Fahim at the group, they used a two-layer neural network based on the just the 2000 most common words, using ReLU activation.  \n",
    "(I think they said their approach was inspired by someone at Kaggle.)\n",
    "Let's try something similar, with initially a single layer leaky ReLU layer, but after using a Truncated SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Deep Network\n",
    "\n",
    "Another idea is to build a deep neural network on the term-frequency matrix, effectively running with extensions to the Naive Bayes model.\n",
    "This will use the reduced term-frequency matrix after the Truncated SVD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, l2_regularizer\n",
    "from tensorflow.contrib.rnn import BasicRNNCell,LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error encountered when serializing regularization_losses.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'function' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter #10000. Current log-loss:0.028003334999084473\n",
      "(956, 1) (956, 1)\n",
      "True Positive 0.08577405857740586. False Positive 0.005230125523012552\n",
      "False Negative 0.010460251046025104. True Negative 0.8985355648535565\n",
      "Log-loss is 0.5419305898648662\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#just use the default graph\n",
    "Nlayers=4\n",
    "Nhidden=400\n",
    "Nout=1\n",
    "lr = 0.01\n",
    "keep_prob=0.9\n",
    "frac_perc=0.01\n",
    "n_iter=10000\n",
    "\n",
    "Nobs,Nfeature=X_train_trans.shape\n",
    "#only grabbing a fraction of the data\n",
    "Nsub=np.int(Nobs*frac_perc)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#load in the training examples, and their labels\n",
    "X = tf.placeholder(tf.float32,[Nsub,Nfeature],name='X')\n",
    "y = tf.placeholder(tf.float32,[Nsub,Nout],name='y')\n",
    "\n",
    "X2 = tf.nn.l2_normalize(X,dim=1)\n",
    "\n",
    "# #make a hidden layer.  Must be smarter way to scale up.\n",
    "H1 = fully_connected(inputs=X2,num_outputs=Nhidden,\n",
    "       activation_fn=tf.nn.relu,\n",
    "       biases_initializer=tf.zeros_initializer,  \n",
    "    weights_regularizer=l2_regularizer,\n",
    "    biases_regularizer=l2_regularizer)\n",
    "H1_d=tf.nn.dropout(H1,keep_prob)\n",
    "\n",
    "H2 = fully_connected(inputs=H1_d,num_outputs=Nhidden,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    biases_initializer=tf.zeros_initializer ,\n",
    "    weights_regularizer=l2_regularizer,\n",
    "    biases_regularizer=l2_regularizer)\n",
    "H2_d=tf.nn.dropout(H2,keep_prob)\n",
    "\n",
    "H3 = fully_connected(inputs=H2_d,num_outputs=Nhidden,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    biases_initializer=tf.zeros_initializer ,\n",
    "    weights_regularizer=l2_regularizer,\n",
    "    biases_regularizer=l2_regularizer)\n",
    "H3_d=tf.nn.dropout(H3,keep_prob)\n",
    "\n",
    "H4 = fully_connected(inputs=H3_d,num_outputs=Nhidden,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    biases_initializer=tf.zeros_initializer ,\n",
    "    weights_regularizer=l2_regularizer,\n",
    "    biases_regularizer=l2_regularizer)\n",
    "H4_d =tf.nn.dropout(H4,keep_prob)\n",
    "\n",
    "#Need to add dropout layers too.\n",
    "\n",
    "# #just condense the number of inputs down, acting as a linear matrix combining results\n",
    "outputs=fully_connected(inputs=H3,num_outputs=Nout,\n",
    "     activation_fn=tf.sigmoid)\n",
    "\n",
    "#should compute mean log-loss\n",
    "eps=1E-15\n",
    "loss = tf.losses.log_loss(y,outputs,epsilon=eps)\n",
    "#loss = tf.reduce_mean(tf.square(y-outputs2))\n",
    "#define optimization function.\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=lr)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "#save model and graph\n",
    "saver=tf.train.Saver()\n",
    "\n",
    "print('Running this thang')\n",
    "with tf.Session() as sess:\n",
    "     init.run()\n",
    "     for iteration in range(n_iter+1):\n",
    "         #select random starting point.\n",
    "         ind_batch,X_batch,y_batch=get_subset(\n",
    "         frac_perc,X_train_trans,actual)\n",
    "         if iteration%100 ==0:\n",
    "            clear_output(wait=True)\n",
    "            mse =loss.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "            print('iter #{}. Current log-loss:{}'.format(iteration,mse))\n",
    "            nn_pred=sess.run(outputs,feed_dict={X:X_batch})\n",
    "            nn_pred_reduced=np.round(nn_pred).astype(bool)\n",
    "            check_predictions(nn_pred_reduced,y_batch)\n",
    "            print('\\n')\n",
    "            #save the weights\n",
    "            saver.save(sess,'tf_models/deep_relu_drop',global_step=iteration)\n",
    "         sess.run(training_op, feed_dict={X: X_batch, y:y_batch})\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Predictions from Deep Neural Network\n",
    "\n",
    "Let's now run some predictions on the full training and development sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#run some predictions by loading the meta-graph.\n",
    "def network_predict(model_name,input_data):\n",
    "    \"\"\"network_predict\n",
    "    Load a saved Neural network, and predict the output labels\n",
    "    based on input_data\n",
    "    \n",
    "    Input: model_name - string name to where model/variables are saved.\n",
    "    input_data - transformed data of shape (Nobs,Nfeature).\n",
    "\n",
    "    Output nn_pred_reduced - vector of predicted labels.\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        loader=tf.train.import_meta_graph(model_name+'.meta')\n",
    "        loader.restore(sess,model_name)\n",
    "        Nobs,Nfeature=input_data.shape\n",
    "        nn_pred_total=np.zeros((Nobs,1))\n",
    "        i0=0\n",
    "        i1=Nsub\n",
    "        while (i1 < Nobs):\n",
    "            X_batch=input_data[i0:i1]\n",
    "            nn_pred=sess.run(outputs,feed_dict={X:X_batch})\n",
    "            nn_pred_total[i0:i1]=nn_pred\n",
    "            i0=i1\n",
    "            i1+=Nsub\n",
    "        #last iter: do remaining operations.  \n",
    "        X_batch=input_data[-Nsub:]\n",
    "        nn_pred=sess.run(outputs,feed_dict={X:X_batch})\n",
    "        nn_pred_total[-Nsub:]=nn_pred\n",
    "        nn_pred_reduced=np.round(nn_pred_total).astype(bool)\n",
    "    return nn_pred_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[ 0.08485558,  0.01652176],\n         [ 0.01175647,  0.8868662 ]]), 0.97670853455007245)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 layer ReLU network\n",
      "(95692, 1) (95692, 1)\n",
      "True Positive 0.09009112569493792. False Positive 0.0037516197801279105\n",
      "False Negative 0.006520921289135978. True Negative 0.8996363332357982\n",
      "Log-loss is 0.3548039987843783\n",
      "Naive Bayes\n",
      "(95692, 1) (95692, 1)\n",
      "True Positive 0.08485557831375663. False Positive 0.01652175730468587\n",
      "False Negative 0.011756468670317268. True Negative 0.8868661957112403\n",
      "Log-loss is 0.9767085345500724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_models/deep_relu_drop-10000\n"
     ]
    }
   ],
   "source": [
    "model_name='tf_models/deep_relu_drop-{}'.format(n_iter)\n",
    "\n",
    "nn_pred_train = network_predict(model_name,X_train_trans)\n",
    "\n",
    "print('3 layer ReLU network')\n",
    "check_predictions(nn_pred_train,actual)\n",
    "print('Naive Bayes')\n",
    "check_predictions(pred_nb,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 layer ReLU network: on Dev set\n",
      "(32128, 1) (32128, 1)\n",
      "True Positive 0.05876494023904383. False Positive 0.0297871015936255\n",
      "False Negative 0.03675921314741036. True Negative 0.8746887450199203\n",
      "Log-loss is 2.2984521024358737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_models/deep_relu-10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 layer ReLU network: on Dev set\n",
      "(32128, 1) (32128, 1)\n",
      "True Positive 0.056710657370517926. False Positive 0.023157370517928287\n",
      "False Negative 0.038813496015936255. True Negative 0.8813184760956175\n",
      "Log-loss is 2.1404164187859576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_models/deep_relu_drop-10000\n"
     ]
    }
   ],
   "source": [
    "#Try testing on the dev-set\n",
    "model_name='tf_models/deep_relu_drop-{}'.format(n_iter)\n",
    "nn_pred_dev = network_predict(model_name,X_dev_trans)\n",
    "print('3 layer ReLU network: on Dev set')\n",
    "actual_dev=df_dev['toxic'].values\n",
    "nn_stats=check_predictions(nn_pred_dev,actual_dev)\n",
    "\n",
    "model_name='tf_models/deep_relu-{}'.format(n_iter)\n",
    "nn_pred_dev = network_predict(model_name,X_dev_trans)\n",
    "print('3 layer ReLU network: on Dev set')\n",
    "actual_dev=df_dev['toxic'].values\n",
    "nn_stats=check_predictions(nn_pred_dev,actual_dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes on Dev set\n",
      "(32128, 1) (32128, 1)\n",
      "True Positive 0.06296688247011953. False Positive 0.023063994023904383\n",
      "False Negative 0.03255727091633466. True Negative 0.8814118525896414\n",
      "Log-loss is 1.9211088744833538\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes on Dev set')\n",
    "pred_dev_nb=nb.predict(X_dev_counts)\n",
    "nb_stats=check_predictions(pred_dev_nb,actual_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Evidently the (3-layer ReLU-tanh) network is over-fitting to the training set.  Not surprising, since there is no regularization here.\n",
    "It could outperform the Naive Bayes method on the training set, but had worse performance on the development dataset.\n",
    "\n",
    "Let's put in some dropout. Putting in dropout after each layer, with a 0.1 dropout probability improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.63848495096381463, 0.69363963655066008]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dev scores\n",
    "[f1_score(actual_dev,nn_pred_dev),f1_score(actual_dev,pred_dev_nb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94606310013717421, 0.85717301805130364]\n",
      "[0.63848495096381463, 0.69363963655066008]\n"
     ]
    }
   ],
   "source": [
    "print([f1_score(actual,nn_pred_train),f1_score(actual,pred_nb)])\n",
    "print([f1_score(actual_dev,nn_pred_dev),f1_score(actual_dev,pred_dev_nb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True], dtype=bool)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.round([1.0, 0.0, 0.1, 0.9])).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(956, 1)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "I am finding that beyond one or two layers, the network just seems to output zeros.  Maybe the learning rate was too high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `sklearn` not found.\n"
     ]
    }
   ],
   "source": [
    "?sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb754de748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checks output for a single training batch.\n",
    "plt.figure()\n",
    "plt.hist(nn_pred[y_batch[:,0],0],bins=20)\n",
    "plt.hist(nn_pred[~y_batch[:,0],0],bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb75311e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(nn_pred_total,bins=100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "init_explore.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
