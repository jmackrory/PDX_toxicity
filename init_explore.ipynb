{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Toxicity in Wikipedia Comments\n",
    "\n",
    "This is a parallel work to any work on the wikipedia toxicity data on the same\n",
    "topic.  This data has not been cleaned yet, and has not had multiple categories introduced yet.  However it is presented free from bias, for people to play with.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159686, 7) (1598289, 4)\n"
     ]
    }
   ],
   "source": [
    "#df_com = pd.read_csv('data/toxicity_annotated_comments_unanimous.tsv',sep='\\t')\n",
    "#df_rate = pd.read_csv('data/toxicity_annotations_unanimous.tsv',sep='\\t')\n",
    "df_com = pd.read_csv('data/toxicity_annotated_comments.tsv',sep='\\t')\n",
    "df_rate = pd.read_csv('data/toxicity_annotations.tsv',sep='\\t')\n",
    "\n",
    "#make rev_id an integer\n",
    "df_com['rev_id']=df_com['rev_id'].astype(int)\n",
    "df_rate['rev_id']=df_rate['rev_id'].astype(int)\n",
    "\n",
    "#reindex \n",
    "#df_com.index=df_com['rev_id']\n",
    "#df_rate.index=df_rate['rev_id']\n",
    "print(df_com.shape, df_rate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#make a new column in df_com with array of worker_ids, and toxicity\n",
    "df_com['scores']=None\n",
    "\n",
    "#since 'rev_id' is sorted, can take first difference, and find where\n",
    "#there are changes.  Those set the boundaries for changes.\n",
    "#Need to also append final index.\n",
    "change_indices=df_rate.index[df_rate['rev_id'].diff()!=0].values\n",
    "\n",
    "#use numpy split instead.\n",
    "arr=df_rate[['worker_id','toxicity_score']].values\n",
    "split_arr=np.split(arr,change_indices)\n",
    "#drop first index as empty\n",
    "split_arr.pop(0)\n",
    "df_com['scores']=split_arr\n",
    "#change_indices=np.append(change_indices,len(df_rate))\n",
    "# for i in range(len(change_indices)-1):\n",
    "#     ind0 = change_indices[i]\n",
    "#     ind1 = change_indices[i+1]\n",
    "#     d0=df_rate.iloc[ind0:ind1]\n",
    "#     scores=d0[['worker_id','toxicity_score']]\n",
    "#     #pass it a list so it can be set as an entry.\n",
    "#     #accessing later will require score[0] idiocy to get at list.\n",
    "#     df_com.loc[i,'scores']=[scores.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_com['mean_toxic']=df_com['scores'].apply(score_mean)\n",
    "df_com['median_toxic']=df_com['scores'].apply(score_median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_com['toxic']=df_com['median_toxic']==-2\n",
    "train_msk=df_com['split']=='train'\n",
    "df_train=df_com[train_msk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def score_mean(score_list):\n",
    "    \"\"\"score_mean\n",
    "    Compute mean of toxicity scores for input array.\n",
    "    Array is first (and only) element in the input list.\n",
    "    Compute mean running down the rows.  Could be updated to include weighted sum of weights\n",
    "    \"\"\"\n",
    "    s = np.mean(score_list[:,1])\n",
    "    return s\n",
    "\n",
    "def score_median(score_list):\n",
    "    \"\"\"score_median\n",
    "    Compute median of toxicity scores for input array.\n",
    "    Array is first (and only) element in the input list.\n",
    "    Compute mean running down the rows.  Could be updated to include weighted sum of weights\n",
    "    \"\"\"\n",
    "    s = np.median(score_list[:,1])\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments: 159686. Toxic comments: 1610. Toxic Fraction: 0.01008228648723119\n"
     ]
    }
   ],
   "source": [
    "Ntoxic=df_com['toxic'].sum()\n",
    "Ntot=len(df_com)\n",
    "print(\"Total comments: {}. Toxic comments: {}. Toxic Fraction: {}\".format(Ntot,Ntoxic,Ntoxic/Ntot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#can combine the dataframes together by extracting all reviewer ids, and scores for each "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Looking at the comments data, we'll need to clean the data quite a bit (lots of newlines, weird characters).\n",
    "\n",
    "My rough plan is to build up a lexicon, tokenize that data, and try to build a Naive Bayes model.  (Maybe later a Recurrent Neural network model?)\n",
    "\n",
    "Other Analysis possibilities:\n",
    "* Support Vector Machine\n",
    "    - the other big architecture, less popular now?\n",
    "* Recurrent Neural Network\n",
    "    - Build up word embeddings (word2vec), or just use the pretrained ones.\n",
    "* Naive Bayes\n",
    "    - can find most important words in spam\n",
    "    - simple, easy to understand baseline.\n",
    "* Latent Factor Analysis \n",
    "    - maybe useful prelude for building up embeddings.\n",
    "    \n",
    "Cleaning:\n",
    "* Tokenize (convert words to indices)\n",
    "* Clean data : How to remove newlines (search/replace: NEWLINE with '')\n",
    "* Stemming words\n",
    "* Balancing data set\n",
    "* Match up comments, and review scores\n",
    "\n",
    "* Search for gibberish words (make a new \"feature\" for badly spelled comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#cleaning the data\n",
    "#Can use pandas built in str functionality with regex to eliminate\n",
    "\n",
    "#maybe also dates?\n",
    "def clean_up(comments):\n",
    "    com_clean=comments.str.replace('NEWLINE_TOKEN',' ')\n",
    "    com_clean=com_clean.str.replace('TAB_TOKEN',' ')    \n",
    "    #Remove HTML trash, via non-greedy replacing anything between backticks\n",
    "    com_clean=com_clean.str.replace(\"style=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"class=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"width=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"align=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"cellpadding=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"cellspacing=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"rowspan=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"colspan=\\`\\`.*?\\`\\`\",' ')\n",
    "    #remove numbers\n",
    "    com_clean=com_clean.str.replace(\"[0-9]+\",' ')\n",
    "    #remove numbers\n",
    "    com_clean=com_clean.str.replace(\"_\",' ')\n",
    "    #remove symbols.    \n",
    "    com_clean=com_clean.str.replace(\"\\[\\[\",' ')\n",
    "    com_clean=com_clean.str.replace(\"\\]\\]\",' ')\n",
    "    com_clean=com_clean.str.replace(\"==\",' ')\n",
    "    com_clean=com_clean.str.replace(\"|\",' ')                                \n",
    "    com_clean=com_clean.str.replace(\"::\",' ')                                    \n",
    "    #remove multiple spaces, replace with a single space\n",
    "    com_clean=com_clean.str.replace('\\\\s+',' ')\n",
    "\n",
    "    #remove symbols\n",
    "    return com_clean\n",
    "df_com['comment_clean']=clean_up(df_com['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95692"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_com['split']=='train').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         This: :One can make an analogy in mathematical terms by envisioning the distribution of opinions...\n1         ` :Clarification for you (and Zundark's right, i should have checked the Wikipedia bugs page fir...\n3         `This is such a fun entry. Devotchka I once had a coworker from Korea and not only couldn't she ...\n6         ` I fixed the link; I also removed ``homeopathy`` as an exampleit's not anything like a legitima...\n7         `If they are ``indisputable`` then why does the NOAA dispute it? Note that the NOAA is the same ...\n9         ` The concept of ``viral meme`` is not a mainstream academic concept, and only merits the briefe...\n11        `just quick notes, since i don't have the time or background to write well on it... purpose: fun...\n12        `The actual idea behind time-out is to get the parent to cool-off. They are the real problem in ...\n15        ` Gjalexei, you asked about whether there is an ``anti-editorializing`` policy here. There is, a...\n18        `] :: When I'm angry, I can't write from the NPOV. I've let articles I care deeply about languis...\n20        ` =Announcement = I have compared Helga's original version and Helga's condensed version to each...\n21                                                 :: You have a valid point,, thanks... any other opinions? \n22                             Some crossword grids don't have black squares instead some gridlines are bold.\n23        `I do not understand this sentence: ``Several authors have criticised the historical inaccuracie...\n26        ` Sorry, it seems I walked on your feet. If you really want the move, please do it, I will no lo...\n27        ` : At the end of the day, it's all arbitrary. The Earth rotates about an axis. One end is calle...\n31         I think it is . James Clerk Maxwell used a color separation method to take three b/w photos thr...\n32                         ok i'll come back. But i get annoyed when ppl have more power than others. - fonzy\n34         Hello there, welcome to the 'pedia! I hope you like the place and decide to stay. If you need a...\n35        ` : To attempt to briefly several of Ed's questions from what I can only hope is an Eastern Orth...\n38        `I've heard it said that Charles Darwin did not want to publish his ideas because he felt it wou...\n39                                                           Thanks for reverting recently damaged articles. \n40        Or, they could change the school schedules to a more appropriate time without changing every clo...\n41         Well, Ayn Rand certainly wasn't a libertarian socialist in history. She was a objectivist, much...\n42        Ed, I am tired! Anyway, I certainly did not mean to trivialize the suffering of vicitms of racis...\n46        ` Span occurs not as a separate page but as an alternate word for ``generate`` on the Linear Alg...\n47                                                   Oh, never mind. I misread the change. My mistake. Egern \n48         Ummm. The article uses imperial measurements, not SI ones. Some translation is apparently in or...\n53                                                                                - Need more on Hephaestion.\n54         ::I disagree. I would leave it out of the introductory part, since its population (which change...\n                                                         ...                                                 \n159643                                                   polar bears are completely purple and have scales.` \n159645                                 what ever you fuggin fag Question how did you know they were not mine \n159646                      Nice try but no cigar........idiot Then explain Odocoileus virginianus idahoensis\n159647                     ` kys { Master Fag - Mind your own damn biz those pages had no quarrel with you }`\n159648     hi Drmies My name's Little Cletus, I'm just here to teach you about Internet manners. For start...\n159651                                        :::Yeah, it's kind of silly. Ha, no good deed goes unpunished. \n159653                           ::I've never attacked you. If you do it again, I'm bringing it to WP:ANI. / \n159655     defunct? The article sources a claim that WOJAC ceased to function in to an article by Shenhav,...\n159656     International goals lists Please, be sensible. You cannot lump together dozens of these lists a...\n159657     Proposal to Rename Article stock market selloff should be renamed to - stock market selloffs be...\n159660    , PA Mitch Fraas - Library Company of Philadelphia Cassatt House Locust Street Philadelphia, PA ...\n159662     :There's some weaseling and pov pushing in the next to last sentence, but other than that I don...\n159663                                    ` :::Yeah and in the earlier sentence I'd reword ``fever pitch``. `\n159664    ` :::::::::::::Again, WP:NOTAFORUM (and there's usually a good reason why a website is blocked o...\n159665                           Where are you? I'm a bit slow in the uptake, apparently, but where are you? \n159666      Japanese Scene The largely neoclassical Japanese power metal scene should be mentioned somewhere.\n159667    ` Jim Hi. How did you get involved in this business with ? Jim left an erroneous and damaging Ed...\n159668     Why oh why... You removed the trolls ANI section about Drmies.....WHILE I WAS EDITING IT, and I...\n159669     Daily Beast Article I'm removing the source that is a hit-piece on RT by The Daily Beast, becau...\n159670    `The lead also lacks proper citation and sources. Saying that Gamergate is a ``Campaign of Haras...\n159671    ` The lead itself is original research. Where is the (proper) citation for Gamergame being a ``h...\n159672                                              :::::long story short. He thinks you are bullying him. - \n159673    ` :List of St. Louis Rams seasons It appears that you tried to give List of St. Louis Rams seaso...\n159674                                                                             :::::: Well done, thanks! \n159675    ` Page patrolled Thanks for sending me an e-mail. But, I use e-mail only for private discussion,...\n159678    ` :``Comment````. Gentlemen, this article provides an insight of the Albanian version of said hi...\n159679     *Support and recommend moving this (and my remark) to AFD, where in lieu of outright deletion y...\n159680    ` File:Romantic Warriors cover.jpg You've tagged File:Romantic Warriors cover.jpg for deletion b...\n159681    ` These sources don't exactly exude a sense of impartiality. Newsweek is a *better* start, but i...\n159684     Warning There is clearly a protectionist regime going on here and a witch hunt to boot. The duc...\nName: comment_clean, Length: 95692, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_com[df_com['split']=='train']['comment_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95692, 125568)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#borrowing from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "count_vect=CountVectorizer(stop_words='english',lowercase=True,strip_accents='unicode')\n",
    "X_train_counts=count_vect.fit_transform(df_train['comment_clean'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cond_prob(X_counts,toxic,csmooth=1):\n",
    "    \"\"\"bayes_prob\n",
    "    X_counts - sparse matrix of counts of each word in a given message\n",
    "    toxic - whether word was toxic or not, with 0,1\n",
    "    \"\"\"\n",
    "    nrows,nwords=X_counts.shape\n",
    "    ptoxic = np.sum(toxic)/nrows\n",
    "    \n",
    "    toxic_mat=X_counts[toxic==1,:]\n",
    "    clean_mat=X_counts[toxic==0,:]\n",
    "    #sum across messages\n",
    "    nword_toxic=np.sum(toxic_mat,axis=0)\n",
    "    nword_clean=np.sum(clean_mat,axis=0)    \n",
    "\n",
    "    #estimate probability of word given toxicity by number of times\n",
    "    #that word occurs in toxic documents, divided by the total number of words\n",
    "    #in toxic documents\n",
    "    #Laplace smooth version\n",
    "    pword_toxic= (nword_toxic+csmooth) \\\n",
    "                / (np.sum(toxic_mat)+nwords*csmooth)\n",
    "\n",
    "    pword_clean= (nword_clean+csmooth) \\\n",
    "                /(np.sum(clean_mat)+nwords*csmooth)\n",
    "    x1=np.sum(toxic_mat,0)\n",
    "    x2=nword_toxic\n",
    "    return ptoxic,pword_toxic,pword_clean    \n",
    "\n",
    "ptox,pw_tox,pw_cln = cond_prob(X_train_counts,df_train['toxic'].values,csmooth=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.2633656852529483e-07, 5.9284202538549556e-06]\n"
     ]
    }
   ],
   "source": [
    "#Version from spam code.\n",
    "# Calculate probabilities that a given word occurs in spam emails.\n",
    "# Also calculate probability that emails are spam.  \n",
    "def calc_prob(word_mat,cat_vec):\n",
    "    #Find dimensions of matrix\n",
    "    nrows,nwords=word_mat.shape\n",
    "    #Training uses the frequency of words.\n",
    "    #Based on previous parts of the question, I was just using whether\n",
    "    # a word occured.  (Following the notes?)\n",
    "    # spamword_occured=(word_full[cat_vec==1,:]>0)\n",
    "    # word_occured=(word_full[cat_vec==0,:]>0)\n",
    "    spam_mat=word_mat[cat_vec==1,:]\n",
    "    ham_mat=word_mat[cat_vec==0,:]\n",
    "\n",
    "    #another fuckup in my probabilities?  I computed the number of spam emails.\n",
    "    #should compute prob of word occuring in spam based on total words in spam.\n",
    "    spamword_num=np.sum(spam_mat)\n",
    "    hamword_num=np.sum(ham_mat)\n",
    "    #Then find proportion of spam emails where each word occurs.\n",
    "    #Fixed version.  should be smooth version of prob(word|spam)\n",
    "    prob_spamword=(np.sum(spam_mat,0)+1)/(spamword_num+nwords)\n",
    "    prob_hamword=(np.sum(ham_mat,0)+1)/(hamword_num+nwords)\n",
    "\n",
    "    #probability for messages to be spam.\n",
    "    pspam = np.sum(cat_vec)/nrows\n",
    "    #Return probability for word in not spam email, prob for word in spam,\n",
    "    #and probability that email is spam.\n",
    "    print([1/(hamword_num+nwords),1/(spamword_num+nwords)])\n",
    "    return pspam,prob_spamword,prob_hamword\n",
    "\n",
    "ptox2,pw_tox2,pw_cln2=calc_prob(X_train_counts,df_train['toxic'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Checking the vectorizer and finding common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#get vocabulary dictionary\n",
    "voc_dict=count_vect.vocabulary_\n",
    "#make a dataframe, with entries as rows\n",
    "voc_df=pd.DataFrame.from_dict(voc_dict,orient='index')\n",
    "#sort by row entry value, and then use that as the index for the counts.\n",
    "voc_df1=voc_df.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 count       p_clean       p_toxic\n",
      "fuck            6179.0  5.836751e-04  1.005937e-01\n",
      "suck            2669.0  1.853770e-04  4.787399e-02\n",
      "faggot          2271.0  3.493221e-04  2.803929e-02\n",
      "die             2062.0  3.483017e-04  2.339616e-02\n",
      "bitch           1284.0  1.489826e-04  1.906859e-02\n",
      "sucks           1282.0  1.486424e-04  1.904605e-02\n",
      "fucking         1938.0  3.904784e-04  1.780638e-02\n",
      "ass             1275.0  2.142885e-04  1.453816e-02\n",
      "fucksex          624.0  3.401351e-09  1.406483e-02\n",
      "asshole          795.0  1.112276e-04  1.054868e-02\n",
      "fucker           537.0  2.381285e-05  1.052614e-02\n",
      "wikipedia      29767.0  9.966641e-03  1.048106e-02\n",
      "shit            1689.0  4.207505e-04  1.018805e-02\n",
      "dick            1022.0  2.000028e-04  9.782341e-03\n",
      "huge             893.0  1.608873e-04  9.466789e-03\n",
      "cocksucker       425.0  6.806102e-06  9.128697e-03\n",
      "mothjer          391.0  3.401351e-09  8.813145e-03\n",
      "cock             561.0  5.782636e-05  8.813145e-03\n",
      "rape             747.0  1.292547e-04  8.272199e-03\n",
      "dog              723.0  1.214316e-04  8.249659e-03\n",
      "offfuck          360.0  3.401351e-09  8.114423e-03\n",
      "penis           1032.0  2.292544e-04  8.069344e-03\n",
      "mexicans         377.0  6.465967e-06  8.069344e-03\n",
      "anal             447.0  3.537745e-05  7.731252e-03\n",
      "niggas           340.0  1.704077e-06  7.550937e-03\n",
      "dickhead         389.0  1.939110e-05  7.483319e-03\n",
      "bastard          474.0  5.102366e-05  7.303003e-03\n",
      "stupid          1374.0  3.574853e-04  7.280464e-03\n",
      "nigger          1238.0  3.156487e-04  6.987451e-03\n",
      "yourselfgo       309.0  3.401351e-09  6.964912e-03\n",
      "...                ...           ...           ...\n",
      "galvan             1.0  3.435364e-07  2.253944e-07\n",
      "gambrel            1.0  3.435364e-07  2.253944e-07\n",
      "gambia            16.0  5.445562e-06  2.253944e-07\n",
      "gambotto           1.0  3.435364e-07  2.253944e-07\n",
      "gambone            1.0  3.435364e-07  2.253944e-07\n",
      "gambon             1.0  3.435364e-07  2.253944e-07\n",
      "gamblinmonkey      1.0  3.435364e-07  2.253944e-07\n",
      "gambling          28.0  9.527183e-06  2.253944e-07\n",
      "gamblin            2.0  6.836715e-07  2.253944e-07\n",
      "gamblers           1.0  3.435364e-07  2.253944e-07\n",
      "gambler            4.0  1.363942e-06  2.253944e-07\n",
      "gamble            16.0  5.445562e-06  2.253944e-07\n",
      "gambits            2.0  6.836715e-07  2.253944e-07\n",
      "gambit             6.0  2.044212e-06  2.253944e-07\n",
      "gambian            3.0  1.023807e-06  2.253944e-07\n",
      "gambare            1.0  3.435364e-07  2.253944e-07\n",
      "galvanic           2.0  6.836715e-07  2.253944e-07\n",
      "gamasutra          1.0  3.435364e-07  2.253944e-07\n",
      "gamarjoba          1.0  3.435364e-07  2.253944e-07\n",
      "gamaliel          98.0  3.333664e-05  2.253944e-07\n",
      "gamal              2.0  6.836715e-07  2.253944e-07\n",
      "gamache            1.0  3.435364e-07  2.253944e-07\n",
      "gama               3.0  1.023807e-06  2.253944e-07\n",
      "gam                3.0  1.023807e-06  2.253944e-07\n",
      "galyan             1.0  3.435364e-07  2.253944e-07\n",
      "galway             1.0  3.435364e-07  2.253944e-07\n",
      "galveston          8.0  2.724482e-06  2.253944e-07\n",
      "galvanized         1.0  3.435364e-07  2.253944e-07\n",
      "galvanised         1.0  3.435364e-07  2.253944e-07\n",
      "ðŒ´ðŒ¹                 1.0  3.435364e-07  2.253944e-07\n",
      "\n",
      "[125568 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "word_mat=np.array([X_train_counts.sum(axis=0),pw_cln,pw_tox]).squeeze()\n",
    "word_df=pd.DataFrame(word_mat.T,columns=['count','p_clean','p_toxic'],index=voc_df1.index)\n",
    "word_df.sort_values('p_toxic',ascending=False,inplace=True)\n",
    "print(word_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "xtot=X_train_counts.sum(axis=0).squeeze()\n",
    "#compare vectorized vs. naive counts to check mappings\n",
    "def check_vect(count_mat,comments,vocab,word):\n",
    "    \"\"\"check_vect(count_mat,comments,vocab,word)\n",
    "    Checks the counts/occurence of words between the count vectorizer,\n",
    "    and a naive 'contains' search.  Returns all the matching comments,\n",
    "    and any discrepencies.        \n",
    "    \"\"\"\n",
    "    ind=vocab.loc[word].values\n",
    "    xtot=count_mat.sum(axis=0)\n",
    "    vect_count=(xtot[0,ind])\n",
    "    #find comments with words\n",
    "    msk=(count_mat[:,ind]>0).toarray().squeeze()\n",
    "    #find comments via naive search\n",
    "    naive_msk=comments.str.contains('{}'.format(word),case=False)\n",
    "    naive_count=np.sum(naive_msk)\n",
    "    comments=comments[msk]\n",
    "    naive_comments=comments[naive_msk]\n",
    "    diff_comments=comments[msk!=naive_msk]\n",
    "    return vect_count,naive_count,comments,naive_comments,diff_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vc,cc,com,ncom,dcom=check_vect(X_train_counts,df_train['comment_clean'],voc_df,'gay')\n",
    "#searching for 'fuck' gives a salutory lesson in why accent tripping is worthwhile, and a simple word filter will probably be circumvented.\n",
    "#does not account for leetspeak 13375|o34|k (but who uses that these days?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vect: [[1866]], Naive: 638\n",
      "[588     ` Pro-Gay Bias? To be honest, I am not sure I entirely follow. What is exactly is meant by holdi...\n",
      "790     ` There is one other thing that just occured to me: an encyclopedia deals in facts, not in specu...\n",
      "871     ` Dante, I agree that it is the use of the word that constructs a POV. Of course, in a technical...\n",
      "1443    Stop editing it. I'm from Santa Clarita, I went to Saugus High School so I know that they are, i...\n",
      "1467     WHY ARE YOU SUCH A GAY NIGGER?!?! GOD DAMNDD... YOU ARE SUCH A GAY NIGGER. FUCK FUCK SHTAY AWAY...\n",
      "Name: comment_clean, dtype: object, 588     ` Pro-Gay Bias? To be honest, I am not sure I entirely follow. What is exactly is meant by holdi...\n",
      "790     ` There is one other thing that just occured to me: an encyclopedia deals in facts, not in specu...\n",
      "871     ` Dante, I agree that it is the use of the word that constructs a POV. Of course, in a technical...\n",
      "1443    Stop editing it. I'm from Santa Clarita, I went to Saugus High School so I know that they are, i...\n",
      "1467     WHY ARE YOU SUCH A GAY NIGGER?!?! GOD DAMNDD... YOU ARE SUCH A GAY NIGGER. FUCK FUCK SHTAY AWAY...\n",
      "Name: comment_clean, dtype: object]\n"
     ]
    }
   ],
   "source": [
    "#currently searching for \"jew\", a term that has clean connotations, but can be used as anti-semitic.\n",
    "print('Vect: {}, Naive: {}'.format(vc,cc))\n",
    "print([com.head(),ncom.head()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "I noticed that there are very few obvious racist slurs in the unanimous data set. (lots of sexism, general hate)\n",
    "Weird sociological question on perception of racism, perhaps by american reviewers?\n",
    "\n",
    "(searching for the n-word found these)\n",
    "Also, some ratings seem way off. e.g. the scores for comments 1467, 1657 include some -1s.\n",
    "Someone even thought 1918 was neutral!\n",
    "Wait, 2669 and 2670 are now identical comments. And some raters thought that 2670 was neutral too!  What the hell?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3279])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     count       p_clean   p_toxic\n",
      "kindom              6179.0  5.814706e-04  0.080174\n",
      "fiss                2669.0  1.846977e-04  0.038157\n",
      "rickson             2271.0  3.480150e-04  0.022349\n",
      "uppityness          2062.0  3.469985e-04  0.018648\n",
      "sincerest           1284.0  1.484426e-04  0.015199\n",
      "michelin            1282.0  1.481037e-04  0.015181\n",
      "donk                1938.0  3.890137e-04  0.014193\n",
      "assertion           1275.0  2.134984e-04  0.011588\n",
      "instinctively        624.0  3.388326e-08  0.011211\n",
      "bs                   795.0  1.108321e-04  0.008409\n",
      "carpentry            537.0  2.375217e-05  0.008391\n",
      "wtekni             29767.0  9.928507e-03  0.008355\n",
      "reqmove             1689.0  4.191698e-04  0.008121\n",
      "bhaduria            1022.0  1.992675e-04  0.007798\n",
      "macmanus             893.0  1.603017e-04  0.007547\n",
      "foliage              425.0  6.810535e-06  0.007277\n",
      "identifications      391.0  3.388326e-08  0.007026\n",
      "gmaxwell             561.0  5.763543e-05  0.007026\n",
      "scienceapologiest    747.0  1.287903e-04  0.006594\n",
      "vedas                723.0  1.209971e-04  0.006577\n",
      "panicing             360.0  3.388326e-08  0.006469\n",
      "totoro              1032.0  2.284071e-04  0.006433\n",
      "dank                 377.0  6.471703e-06  0.006433\n",
      "territoriality       447.0  3.527247e-05  0.006163\n",
      "itd                  340.0  1.728046e-06  0.006020\n",
      "weasal               389.0  1.934734e-05  0.005966\n",
      "helen                474.0  5.085877e-05  0.005822\n",
      "bluffton            1374.0  3.561470e-04  0.005804\n",
      "bogdanovicu         1238.0  3.144705e-04  0.005571\n",
      "tute                 309.0  3.388326e-08  0.005553\n",
      "...                    ...           ...       ...\n",
      "wweshop                1.0  3.727159e-07  0.000002\n",
      "lorded                 1.0  3.727159e-07  0.000002\n",
      "cufcatp                1.0  3.727159e-07  0.000002\n",
      "vmglk                  1.0  3.727159e-07  0.000002\n",
      "papineau               1.0  3.727159e-07  0.000002\n",
      "strathcona             1.0  3.727159e-07  0.000002\n",
      "shied                  1.0  3.727159e-07  0.000002\n",
      "arrondissement         1.0  3.727159e-07  0.000002\n",
      "councillers            6.0  2.066879e-06  0.000002\n",
      "midsized               1.0  3.727159e-07  0.000002\n",
      "occasionality          1.0  3.727159e-07  0.000002\n",
      "naledi                 2.0  7.115485e-07  0.000002\n",
      "statu                  1.0  3.727159e-07  0.000002\n",
      "regavim                3.0  1.050381e-06  0.000002\n",
      "rosoft                 1.0  3.727159e-07  0.000002\n",
      "twatbags               2.0  7.115485e-07  0.000002\n",
      "storks                 1.0  3.727159e-07  0.000002\n",
      "largoplazo             1.0  3.727159e-07  0.000002\n",
      "villagevoice           1.0  3.727159e-07  0.000002\n",
      "tuli                   5.0  1.728046e-06  0.000002\n",
      "kupferberg             1.0  3.727159e-07  0.000002\n",
      "coucher                1.0  3.727159e-07  0.000002\n",
      "kgwzl                  1.0  3.727159e-07  0.000002\n",
      "iffr                   1.0  3.727159e-07  0.000002\n",
      "gcx                    1.0  3.727159e-07  0.000002\n",
      "jxoc                   1.0  3.727159e-07  0.000002\n",
      "obivous                1.0  3.727159e-07  0.000002\n",
      "aewbwovchmi            2.0  7.115485e-07  0.000002\n",
      "jixwivijmech           1.0  3.727159e-07  0.000002\n",
      "massanari              1.0  3.727159e-07  0.000002\n",
      "\n",
      "[125568 rows x 3 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Looks like my word mapping is completely screwed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9351)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes(mat,pword_tox,pword_cln,ptox):\n",
    "    \"\"\"Compute probability that a message \n",
    "    is toxic via naive_bayes estimate.\n",
    "    \"\"\"\n",
    "    msk = mat>0\n",
    "    #ptox_word = (pword_tox * p_tox)/(pword_cln*pcln + pword_tox * p_tox)\n",
    "    #Need to multiply together.  Can just add the logs, and exponentiate at\n",
    "    #the end\n",
    "\n",
    "    #do once for all words\n",
    "    log_pword = np.log(pword_tox*ptox/(pword_tox*ptox + pword_cln*(1-ptox)))\n",
    "    log_score = mat.dot(log_pword.T)\n",
    "    score=np.exp(log_score)\n",
    "    return score\n",
    "scores=naive_bayes(X_train_counts,pw_tox,pw_cln,ptox)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efba3f0e358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                            comment_clean  \\\n",
       "18                                                                                       :Are you there?    \n",
       "22                                                                                               * Keep.    \n",
       "25                                                                                               * Keep.    \n",
       "73                                                                                                          \n",
       "82                                                                                                     â€”    \n",
       "86                                                                                                ` : : `   \n",
       "139                                                                                      ::He always is.    \n",
       "159                                                                                 What else is on her?    \n",
       "165                                                                                                    /    \n",
       "166                                                                              :What are you on about?    \n",
       "222                                                                                                         \n",
       "229                                                                                                   ` `   \n",
       "318                                                                               And beyond all this, it   \n",
       "350                                                                              (among others) were made   \n",
       "371                                                                                              * - May    \n",
       "441                                                                               :or is it Here, here?.    \n",
       "454                                                         these idiots............ Mitchell Starrenburg   \n",
       "590                                                                                 This is one of them.    \n",
       "771                                                                                                  : ,    \n",
       "778                                        YOU SUCK YOU SUCK YOU SUCK YOU SUCK YOU SUCK YOU SUCK YOU SUCK   \n",
       "843                                                                                       Another one: .    \n",
       "898                                                                    ` :Not the only one either! '''' `   \n",
       "914                                                                        many, many, many, many, many}}   \n",
       "936                                                                          What is it and where is it?    \n",
       "972    FUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OF...   \n",
       "1165                                                                                      :It is here. -    \n",
       "1231                                                                                              ` May `   \n",
       "1266                                                                              :It's not even there??    \n",
       "1283                                                                                                  ` `   \n",
       "1359                                                                                            :Why so?    \n",
       "...                                                                                                   ...   \n",
       "2026                                                                              ` ::::: how about no. `   \n",
       "2062                                                                                     ` ''''' : , May`   \n",
       "2095                                                                         I am still a gigantic faggot   \n",
       "2098                                                                                         Some should    \n",
       "2174                                                                              ` And so what's this? `   \n",
       "2197                                                                                           : Me too!    \n",
       "2256                                                                                            ::done Â·    \n",
       "2275                                                                            ` Another one. '''''' - `   \n",
       "2330                                                                             (After I found out how)    \n",
       "2374                                                                        ::::::This one and this one.    \n",
       "2402                                                                   : I had enough of *you*, bastard.    \n",
       "2426                                                                               do so sexually with me   \n",
       "2459                                                                       ::Been there, done that. -) -    \n",
       "2478                                                                             Bongwarrior is a faggot.   \n",
       "2480                                                                      What what what what is this ???   \n",
       "2591                                                                                         . . . . . .    \n",
       "2618                                                                                         ` : '''''' `   \n",
       "2634                                                                                      And one at . -    \n",
       "2662                                                                                                   ~    \n",
       "2759                                                                                             . . . â€¢    \n",
       "2864                                                                       ` Why Why are you so stupid? `   \n",
       "2958                                                                 Then you're both blind and retarded.   \n",
       "2984                                                                                        ` ::Done! â˜† `   \n",
       "3133                                                                               , as will as the third   \n",
       "3163                                                                                     is , , and could   \n",
       "3206                                                                                                   /    \n",
       "3215                                                                  , how about one of these for you??    \n",
       "3224                                                                             ::Or, perhaps: ::: :: â€”    \n",
       "3234                                                                                            :See . â€”    \n",
       "3328                                                                                           ::You to.    \n",
       "\n",
       "      toxic  \n",
       "18    False  \n",
       "22    False  \n",
       "25    False  \n",
       "73    False  \n",
       "82    False  \n",
       "86    False  \n",
       "139   False  \n",
       "159   False  \n",
       "165   False  \n",
       "166   False  \n",
       "222   False  \n",
       "229   False  \n",
       "318   False  \n",
       "350   False  \n",
       "371   False  \n",
       "441   False  \n",
       "454    True  \n",
       "590   False  \n",
       "771   False  \n",
       "778    True  \n",
       "843   False  \n",
       "898   False  \n",
       "914   False  \n",
       "936   False  \n",
       "972    True  \n",
       "1165  False  \n",
       "1231  False  \n",
       "1266  False  \n",
       "1283  False  \n",
       "1359  False  \n",
       "...     ...  \n",
       "2026  False  \n",
       "2062  False  \n",
       "2095   True  \n",
       "2098  False  \n",
       "2174  False  \n",
       "2197  False  \n",
       "2256  False  \n",
       "2275  False  \n",
       "2330  False  \n",
       "2374  False  \n",
       "2402   True  \n",
       "2426   True  \n",
       "2459  False  \n",
       "2478   True  \n",
       "2480  False  \n",
       "2591  False  \n",
       "2618  False  \n",
       "2634  False  \n",
       "2662  False  \n",
       "2759  False  \n",
       "2864   True  \n",
       "2958   True  \n",
       "2984  False  \n",
       "3133  False  \n",
       "3163  False  \n",
       "3206  False  \n",
       "3215  False  \n",
       "3224  False  \n",
       "3234  False  \n",
       "3328  False  \n",
       "\n",
       "[73 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the messages classified as spam.\n",
    "msk = np.array(scores>0.5)\n",
    "df_com[msk][['comment_clean','toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4887332794685343e-05"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(pw_tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   2.00000000e+00,   1.00000000e+00, ...,   1.00000000e+00,   1.00000000e+00,   1.00000000e+00],\n",
       "       [  5.95663569e-05,   8.93495354e-05,   5.95663569e-05, ...,   5.95663569e-05,   5.95663569e-05,   5.95663569e-05],\n",
       "       [  4.48873328e-05,   4.48873328e-05,   4.48873328e-05, ...,   4.48873328e-05,   4.48873328e-05,   4.48873328e-05]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_mat.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test=['Zep the blah blah','Cow Alt the blah Alt','foo the if']\n",
    "test2=count_vect.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt': 0, 'blah': 1, 'cow': 2, 'foo': 3, 'zep': 4}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 0, 1],\n",
       "       [2, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.sum(axis=0).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#get term frequencies via tfidf transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So, I briefly embarassed myself by looking for racist slurs,\n",
    "and found none of the obvious American candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df_com['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck 57401\n",
      "shit 136209\n",
      "cunt 35357\n",
      "piss 114922\n",
      "cocksucker 29180\n",
      "dick 40351\n",
      "ass 10569\n",
      "asshole 10688\n",
      "bitch 17516\n"
     ]
    }
   ],
   "source": [
    "naughty_word=['fuck','shit','cunt','piss','cocksucker','dick','ass','asshole','']\n",
    "for word in naughty_word:\n",
    "    try:\n",
    "        print(word,count_vect.vocabulary_[word])\n",
    "    except:\n",
    "        print(word,'not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "init_explore.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
