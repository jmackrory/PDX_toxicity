{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Toxicity in Wikipedia Comments\n",
    "\n",
    "Hostile, toxic comments and discussions are a fact of life on the present internet.\n",
    "Algorithms offer one approach to moderating these discussions, to filter out harmful, ugly\n",
    "comments which detract from the conversation. This project is an attempt to explore\n",
    "tha question, and build some simple models to classify online comments as \"toxic\" or \"non-toxic.\"\n",
    "\n",
    "This is using the dataset from\n",
    "\"Wulczyn, Ellery; Thain, Nithum; Dixon, Lucas (2016): Wikipedia Detox. figshare. doi.org/10.6084/m9.figshare.4054689\",\n",
    "which was cleaned up to provide the Kaggle dataset.\n",
    "That data used Amazon Mechanical Turk workers to rate comments taken from Wikipedia talk pages as being\n",
    "toxic, or non-toxic.  This version of the data also includes demographic and scoring information.\n",
    "While this notebook does not explore how these topics are perceived, I would like to explore the variations\n",
    "in how the topics are perceived.  \n",
    "This data has not been cleaned yet, and has not had multiple categories for the variety of toxity introduced yet.\n",
    "\n",
    "This initial analysis looks at the data, and implements tokenization, Naive Bayes, and a Deep Neural Network.\n",
    "There's also some material considering methods for building a SVM.\n",
    "This is succeeded by kaggle_anlt.ipynb, which runs further with the model building, but has less focus on\n",
    "exploration.\n",
    "\n",
    "Beware: Lots of swearing, racism, homophobia, misogyny is contained within due to nature of the comments.\n",
    "And the fact I have searched for nasty terms as a sanity check on how the methods are working.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Outline/Planning\n",
    "\n",
    "Looking at the comments data, we'll need to clean the data quite a bit (lots of newlines, weird characters).\n",
    "There is also a lot of wikipedia markup, and mis-spelled words.\n",
    "\n",
    "My rough plan is to build up a lexicon, tokenize that data, and try to build a Naive Bayes model.  (Maybe later a Recurrent Neural network model?)\n",
    "\n",
    "Cleaning:\n",
    "* Clean data : How to remove newlines (search/replace: NEWLINE with '')  (done)\n",
    "* Tokenize (convert words to indices) (done)\n",
    "* Stemming words\n",
    "* Balancing data set\n",
    "* Match up comments, and review scores (done)\n",
    "* Search for gibberish words (make a new \"feature\" for badly spelled comments)\n",
    "* Spelling: one of the easiest ways to avoid a word based system is to misspell words, \"fck you\".\n",
    "            This does suggest perhaps working with character n-grams, rather than just words. \n",
    "\n",
    "Embeddings:\n",
    "These are necessary to reduce the dimensionality of the problem to a scale that will fit in memory.  \n",
    "   * SVD - use SVD on the term-frequency matrix. Will use truncated SVD.  \n",
    "   * word2vec - train vectors for words based on surrounding contexts (can use pre-trained ones, like GLoVE)\n",
    "   * Latent Factor Analysis - maybe useful prelude or alternative for building up embeddings.\n",
    "                            - ALS is similar to SVD, but not guaranteed to be orthogonal.\n",
    "   * Keep only most common words (in both toxic/non-toxic), or highest probability of toxic/non-toxic\n",
    "\n",
    "Other Analysis possibilities:\n",
    "* Naive Bayes\n",
    "    - can find most important words\n",
    "    - simple, easy to understand baseline.\n",
    "* Support Vector Machine\n",
    "    - try ensemble method (split the data into batches, and train an SVM on each batch.  Then do a committee vote.)\n",
    "      This turns O(n_sample^3) scaling into O(n_sample^3/n_batch^2) scaling on the training.\n",
    "      This is effectively treating the kernel matrix as if it were block-diagonal, as it omits correlations between datasets.\n",
    "      Perhaps running multiple copies with different random splits would work?\n",
    "* Deep Neural Network\n",
    "    - Build a network using the term-frequency matrix as inputs.\n",
    "    - Extends the naive Bayes method.  (Might be automatic way of doing some of that SVM stuff?)\n",
    "    - Employ dropout for regularization, alongside L2 penalties.  \n",
    "     \n",
    "* Recurrent Neural Network\n",
    "    - Build up word embeddings (word2vec), or just use the pretrained ones.\n",
    "    - This one runs at the sentence/paragraph level and keeps the temporal structure.\n",
    "    - Use LSTM/GRU cells, with a couple layers. \n",
    "    - Also dropout, l2 penalties\n",
    "\n",
    "Metrics:\n",
    "    - F1 :harmonic mean of precision and recall\n",
    "    - log-loss $N^{-1}\\sum_{j=1}^N\\sum_c y_{jc}\\log \\hat{y}_{jc}$, where $j$ runs over observations, and $c$ runs over classes.\n",
    "    - AUROC: Something like Gini coefficient?  (Plot the true-positive/false-positive curve as the decision threshold $t$ is varied.)\n",
    "The last two were used as Kaggle metrics.  They just changed over to the column average AUC-ROC metric.  Apparently this is less sensitive to leader-board climbing than the log-loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import log_loss,f1_score,roc_auc_score\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "#my code\n",
    "from bayes import cond_prob, naive_bayes\n",
    "from util import clean_up, get_subset, check_predictions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159686, 7) (1598289, 4)\n"
     ]
    }
   ],
   "source": [
    "df_com = pd.read_csv('data/toxicity_annotated_comments.tsv',sep='\\t')\n",
    "df_rate = pd.read_csv('data/toxicity_annotations.tsv',sep='\\t')\n",
    "\n",
    "#make rev_id an integer\n",
    "df_com['rev_id']=df_com['rev_id'].astype(int)\n",
    "df_rate['rev_id']=df_rate['rev_id'].astype(int)\n",
    "\n",
    "print(df_com.shape, df_rate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rev_id', 'comment', 'year', 'logged_in', 'ns', 'sample', 'split',\n",
       "       'scores', 'mean_toxic', 'median_toxic', 'toxic', 'comment_clean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_com.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbb7b73a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#When are the comments made?\n",
    "plt.figure()\n",
    "bin_arr=np.sort(df_com['year'].unique())\n",
    "df_com['year'].hist(bins=bin_arr)\n",
    "plt.title('Total Comments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#make a new column in df_com with array of worker_ids, and toxicity\n",
    "df_com['scores']=None\n",
    "\n",
    "#since 'rev_id' is sorted, can take first difference, and find where\n",
    "#there are changes in 'rev_id'.  Those set the boundaries for changes.\n",
    "change_indices=df_rate.index[df_rate['rev_id'].diff()!=0].values\n",
    "\n",
    "#use numpy split to split the array into many sub-arrays.\n",
    "arr=df_rate[['worker_id','toxicity_score']].values\n",
    "split_arr=np.split(arr,change_indices)\n",
    "#drop first index as empty\n",
    "split_arr.pop(0)\n",
    "df_com['scores']=split_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def score_mean(score_list):\n",
    "    \"\"\"score_mean\n",
    "    Compute mean of toxicity scores for input array.\n",
    "    Array is first (and only) element in the input list.\n",
    "    Compute mean running down the rows.  Could be updated to include weighted sum of weights\n",
    "    \"\"\"\n",
    "    s = np.mean(score_list[:,1])\n",
    "    return s\n",
    "\n",
    "def score_median(score_list):\n",
    "    \"\"\"score_median\n",
    "    Compute median of toxicity scores for input array.\n",
    "    Array is first (and only) element in the input list.\n",
    "    Compute mean running down the rows.  Could be updated to include weighted sum of weights\n",
    "    \"\"\"\n",
    "    s = np.median(score_list[:,1])\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Make a new column computing mean, median scores for each comment\n",
    "df_com['mean_toxic']=df_com['scores'].apply(score_mean)\n",
    "df_com['median_toxic']=df_com['scores'].apply(score_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#So 373 duplicated comments.  Awesome.  \n",
    "dup_msk=df_com['comment'].duplicated(keep=False)\n",
    "#I'm just going to drop these duplicates\n",
    "df_com.drop_duplicates(subset='comment',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments: 159463. Toxic comments: 15353. Toxic Fraction: 0.09627938769495117\n"
     ]
    }
   ],
   "source": [
    "#Define toxic comments as those where the median is below -1, or -2.\n",
    "#-1 captures more comments, but with more variance in what is considered toxic/unhelpful.\n",
    "df_com['toxic']=(df_com['median_toxic']<=-1)\n",
    "Ntoxic=df_com['toxic'].sum()\n",
    "Ntot=len(df_com)\n",
    "print(\"Total comments: {}. Toxic comments: {}. Toxic Fraction: {}\".format(Ntot,Ntoxic,Ntoxic/Ntot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbb7b58e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#When are the comments made?  Has the toxicity changed over time?\n",
    "#Note this is on the full dataset, with test/training/dev splits. \n",
    "plt.figure(figsize=(10,10))\n",
    "bin_arr=np.sort(df_com['year'].unique())\n",
    "#non-toxic comments\n",
    "plt.subplot(2,2,1)\n",
    "msk1=df_com['median_toxic']<=-1\n",
    "plt.ylabel('Toxicity=-1')\n",
    "df_com['year'][msk1].hist(bins=bin_arr)\n",
    "plt.title('Toxic')\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Non-Toxic')\n",
    "df_com['year'][~msk1].hist(bins=bin_arr)\n",
    "#second row\n",
    "plt.subplot(2,2,3)\n",
    "msk2=df_com['median_toxic']<=-2\n",
    "df_com['year'][msk2].hist(bins=bin_arr)\n",
    "plt.ylabel('Toxicity=-2')\n",
    "plt.subplot(2,2,4)\n",
    "df_com['year'][~msk2].hist(bins=bin_arr)\n",
    "plt.xlabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So the data looks to be evenly balanced as toxic/non-toxic across time, with a rough 10% fraction reduction from regular to toxic, to severely toxic.\n",
    "Either the internet has not really changed much, or the data gatherers carefully kept the data calibrated. \n",
    "Another question about the data is what topics were under discussion? Does this bias the output/findings? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#cleaning the data\n",
    "#Can use pandas built in str functionality with regex to eliminate html tags, newlines, non-text characters. \n",
    "#Can maybe also eliminate all punctuation?  Makes any \n",
    "\n",
    "#maybe also dates?\n",
    "df_com['comment_clean']=clean_up(df_com['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This does lose some information.  Such as possible rude symbols (There's like 6 of these crude ascii art drawings.  This is probably not worth tracking down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rev_id', 'comment', 'year', 'logged_in', 'ns', 'sample', 'split',\n",
       "       'scores', 'mean_toxic', 'median_toxic', 'toxic', 'comment_clean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_com.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#put the file as tsv \n",
    "df_com.to_csv('saved_dataframes/cleaned_comments.tsv.gzip',sep='\\t',\n",
    "columns=['rev_id','comment_clean','scores','mean_toxic','median_toxic','split','toxic'],compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#read in saved cleaned up dataframe.\n",
    "df_com=pd.read_csv('saved_dataframes/cleaned_comments.tsv.gzip',sep='\\t',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#separate off training_split\n",
    "train_msk=df_com['split']=='train'\n",
    "df_train=df_com[train_msk]\n",
    "df_dev=df_com[df_com['split']=='dev']\n",
    "df_test=df_com[df_com['split']=='test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So the following vectorizer eliminates the stop words.  However, while stop words have little impact\n",
    "on the semantic content of regular documents, in internet toxicity they are absent from the most toxic messages.\n",
    "However, the hardest to classify comments will probably have them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "It's possible to select the n-gram range.  I've found that using 2-grams tends to lead to over-fitting (and a huge corpus).\n",
    "This will just use 1-grams.  I'll also not use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#borrowing from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "count_vect=CountVectorizer(stop_words='english',lowercase=True,strip_accents='unicode')\n",
    "tfidf_vect=TfidfVectorizer(stop_words='english',lowercase=True,strip_accents='unicode')\n",
    "X_train_counts=count_vect.fit_transform(df_train['comment_clean'])\n",
    "X_train_tfidf=tfidf_vect.fit_transform(df_train['comment_clean'])\n",
    "\n",
    "#do the same transformations using existing vocab built up in training.\n",
    "X_dev_tfidf=tfidf_vect.transform(df_dev['comment_clean'])\n",
    "X_test_tfidf=tfidf_vect.transform(df_test['comment_clean'])\n",
    "\n",
    "X_dev_counts=count_vect.transform(df_dev['comment_clean'])\n",
    "X_test_counts=count_vect.transform(df_test['comment_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Spell checking \n",
    "\n",
    "Let's try using pyenchant to spellcheck these messages.\n",
    "My goal is to build a way of catching inventive attempts to circumvent the\n",
    "swearing filter.  (This catches any idiocy like \"fck you\")\n",
    "Anything not recognize will count as a spelling error, which could be used as a feature.\n",
    "\n",
    "However, this will also end up penalizing proper names, foreign languages, and rare technical terms.\n",
    "I would hope these would be outweighed by having other genuine content.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from enchant.checker import SpellChecker\n",
    "chkr = SpellChecker(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10105\n"
     ]
    }
   ],
   "source": [
    "#Make a custom dictionary based on data:\n",
    "#Tokenize words.  Accept new words that show up in more than 5 messages.\n",
    "X_log=np.sum(X_train_counts>0,axis=0)\n",
    "\n",
    "msk=X_log>20\n",
    "print(np.sum(msk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word_arr=voc_df1[msk.T].index.values\n",
    "#now check which words are \"new\"\n",
    "new_word=np.zeros(word_arr.shape)\n",
    "for i in range(len(word_arr)):\n",
    "    chkr.set_text(word_arr[i])\n",
    "    for err in chkr:\n",
    "        #print('Error:',err.word)\n",
    "        new_word[i]=1\n",
    "np.sum(new_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#write these to a custom CSV dict for use in checking.\n",
    "new_word_series=pd.Series(word_arr[new_word>0])\n",
    "new_word_series.to_csv('new_word_dict.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#use US english, and augmented by new \"common\" words.\n",
    "chkr = SpellChecker(\"en_US\",\"new_word_dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 7.926937818527222\n"
     ]
    }
   ],
   "source": [
    "Ncheck=1000\n",
    "\n",
    "err_tot=np.zeros(Ncheck)\n",
    "t0=time.time()\n",
    "for i in range(Ncheck):\n",
    "    chkr.set_text(df_com.iloc[i]['comment_clean'])\n",
    "    for err in chkr:\n",
    "        #print('Error:',err.word)\n",
    "        err_tot[i]+=1\n",
    "t1=time.time()\n",
    "print('Time taken:',t1-t0)\n",
    "#So around 8 sec for 1000 entries.  Will be around an hour for the whole set of 10^5.\n",
    "#Could use multiprocess to split up, as this is a trivially parallel task.\n",
    "#But note:chkr is stateful - need a list of independent chkrs for each process/thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Let's try to add features such as checking the fraction of all-caps words, the number of mis-spelled words, and repetition.\n",
    "The spellchecker can find the number of misspelled words, can construct a freaction of the message that is mispelled.\n",
    "Repetition can be estimated from the term-frequency matrix, by taking the ratio of any word to the total number of words.\n",
    "Capitalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Checking the vectorizer and finding common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "I wanted to check that the vectorizer was working by outputting common words, and identifying the \"most toxic\" words, based on their counts.\n",
    "This was useful as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#get vocabulary dictionary, then make a dataframe, with entries as rows\n",
    "#Then sort dataframe by row entry value, and then use that as the index for the counts.\n",
    "voc_dict=count_vect.vocabulary_\n",
    "voc_df=pd.DataFrame.from_dict(voc_dict,orient='index')\n",
    "voc_df1=voc_df.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Compute conditional probabilities of toxicity for each word. \n",
    "ptox,pw_tox,pw_cln = cond_prob( X_train_counts, df_train['toxic'].values, csmooth=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fucksex' 'buttsecks' 'bastered' 'cocksucker' 'fggt' 'mothjer' 'offfuck'\n",
      " 'niggas' 'sexsex' 'yourselfgo' 'marcolfuck' 'fack' 'veggietales'\n",
      " 'notrhbysouthbanof' 'ancestryfuck' 'shitfuck' 'yaaaa' 'cuntbag' 'haahhahahah'\n",
      " 'cuntliz']\n"
     ]
    }
   ],
   "source": [
    "#make new dataframe with conditional probabilities for words being toxic, and raw probabilities of occuring in toxic/clean messages\n",
    "#Then sort by toxicity.\n",
    "X_cond= pw_tox*ptox/(pw_tox*ptox + pw_cln*(1-ptox))\n",
    "word_mat=np.array([X_train_counts.sum(axis=0),X_cond,pw_cln,pw_tox]).squeeze()\n",
    "word_df=pd.DataFrame(word_mat.T,columns=['count','pcond','p_clean','p_toxic'],index=voc_df1.index)\n",
    "word_df.sort_values('pcond',ascending=False,inplace=True)\n",
    "pcond_wds=word_df.head(n=20).index.values\n",
    "\n",
    "#Mis-spelled rare swearing follows.  And someone's vendetta against VeggieTales.\n",
    "print(pcond_wds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So, the most toxic words (i.e. words that only appeared in toxic messages) are misspelled attempts at rudeness, with weird spaces, and combination words.  I think this reflects more on the pre-processing.  These words show up in a single toxic message, and are thus great at inferring that one message is toxic.  This doesn't say much about more general trends in the messages.\n",
    "\n",
    "I am considering also implementing a spell-check, and adding a variable for the number of incorrect words or fraction of the message that is misspelled.  Another feature would be the fraction that is capitalized?\n",
    "The accent stripping catches simple attempts to avoid the spam filter with accents, but does miss things where the words are spaced out, or have other characters inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "xtot=X_train_counts.sum(axis=0).squeeze()\n",
    "#compare vectorized vs. naive counts to check mappings\n",
    "def check_vect(count_mat,comments,vocab,word):\n",
    "    \"\"\"check_vect(count_mat,comments,vocab,word)\n",
    "    Checks the counts/occurence of words between the count vectorizer,\n",
    "    and a naive 'contains' search.  Returns all the matching comments,\n",
    "    and any discrepencies.        \n",
    "    \"\"\"\n",
    "    ind=vocab.loc[word].values\n",
    "    xtot=count_mat.sum(axis=0)\n",
    "    vect_count=(xtot[0,ind])\n",
    "    #find comments with words\n",
    "    msk=(count_mat[:,ind]>0).toarray().squeeze()\n",
    "    #find comments via naive search\n",
    "    naive_msk=comments.str.contains('{}'.format(word),case=False)\n",
    "    naive_count=np.sum(naive_msk)\n",
    "    comments=comments[msk]\n",
    "    naive_comments=comments[naive_msk]\n",
    "    diff_comments=comments[msk!=naive_msk]\n",
    "    return vect_count,naive_count,comments,naive_comments,diff_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#The following searches for words via a naive regex, and compares the results with the tokenizer.\n",
    "vc,cc,com,ncom,dcom=check_vect(X_train_counts,df_train['comment_clean'],voc_df,pcond_wds[0])\n",
    "#searching for 'fuck' gives a salutory lesson in why accent tripping is worthwhile, and a simple word filter will probably be circumvented.\n",
    "# print('Vect: {}, Naive: {}'.format(vc,cc))\n",
    "# print(com.head(),'\\n\\n')\n",
    "# print(ncom.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marcolfuck :\t 260 occurences: \t 1 messages\n",
      "fack :\t 232 occurences: \t 2 messages\n",
      "veggietales :\t 212 occurences: \t 1 messages\n",
      "notrhbysouthbanof :\t 208 occurences: \t 2 messages\n",
      "ancestryfuck :\t 208 occurences: \t 1 messages\n",
      "shitfuck :\t 182 occurences: \t 1 messages\n",
      "yaaaa :\t 128 occurences: \t 1 messages\n",
      "cuntbag :\t 128 occurences: \t 3 messages\n",
      "haahhahahah :\t 128 occurences: \t 1 messages\n",
      "cuntliz :\t 111 occurences: \t 1 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fucksex :\t 624 occurences: \t 1 messages\n",
      "buttsecks :\t 498 occurences: \t 2 messages\n",
      "bastered :\t 449 occurences: \t 2 messages\n",
      "cocksucker :\t 425 occurences: \t 37 messages\n",
      "fggt :\t 398 occurences: \t 5 messages\n",
      "mothjer :\t 391 occurences: \t 4 messages\n",
      "offfuck :\t 360 occurences: \t 1 messages\n",
      "niggas :\t 340 occurences: \t 7 messages\n",
      "sexsex :\t 332 occurences: \t 1 messages\n",
      "yourselfgo :\t 309 occurences: \t 1 messages\n"
     ]
    }
   ],
   "source": [
    "## Naughty words catch obvious candidates, and check for more British insults,\n",
    "## and slurs for arabs.\n",
    "word_counts=X_train_counts.sum(axis=0)\n",
    "for word in pcond_wds:\n",
    "    try:\n",
    "        ind=count_vect.vocabulary_[word]\n",
    "        n_occur=word_counts[0,ind]\n",
    "        n_tot=np.sum(X_train_counts[:,ind]>0)\n",
    "        print(word,':\\t {} occurences: \\t {} messages'.format(n_occur,n_tot))\n",
    "    except:\n",
    "        print(word,'not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So, the conditional probability for a word being toxic is chiefly determined by whether it only occurs in toxic messages.  In this case, these \"most toxic words\" are misspelled or portmanteus.  The high counts are offset by only appearing in few messages.  This suggest these are just lengthy repetitions or a single rude message.\n",
    "\n",
    "A spellcheck might catch these, and correct the spelling?  That would potentially catch the attempts to circumvent obvious mis-spelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As an attempt to make a simpler, smaller data set to work with, Matt Borthwick selected out the comments with unanimous ratings.\n",
    "This revealed an interesting phenomenon: there are very few obvious racist slurs in the unanimous data set. (lots of sexism, general hate)\n",
    "People disagreed quite a bit on the extent to which racism was toxic, which suggests analyzing how the (predominantly American?) reviewers responded to the data. \n",
    "Weird sociological question on perception of toxicity of racism, perhaps by american reviewers? (this is something that the actual original project is explicitly considering at https://conversationai.github.io/bias.html)\n",
    "\n",
    "(searching for the n-word found these)\n",
    "Some ratings seem way off. e.g. the scores for comments 1467, 1657 include some -1s.\n",
    "Someone even thought 1918 was neutral!\n",
    "Wait, 2669 and 2670 are now identical comments. And some raters thought that 2670 was neutral too!  What the hell?!\n",
    "This suggests using the median toxicity score to avoid the mean being contaminated by people with a really different sense of what is toxic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Naive Bayes\n",
    "\n",
    "I want to implement a Naive Bayes classifier as a baseline.  I've written my own version, which I will try to compare to\n",
    "scikit-learn's version.  (They both return the same result now).\n",
    "\n",
    "This basically treats the comments in a bag-of-words sense, and drops any correlations between the words.  Perhaps including some more\n",
    "common n-grams, e.g. \"frigging crank\".\n",
    "\n",
    "* Estimate $p(w|T)$ from counts in term-frequency matrix.\n",
    "* Use Bayes Rule\n",
    "  $ P(T|w) = \\frac{p(T)p(w|T)}{\\text{normalization const}}$\n",
    "\n",
    "  \\begin{equation}\n",
    "    p(T|\\text{words}) = P(T) \\prod_{words}\\frac{p(w_i|T)}{p(w_i|T)\n",
    "  \\end{equation}\n",
    "\n",
    "* Use Logarithms, and compare log-odds for toxicity/non-toxic.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/Data-Science/PDSG_DataScience/PDX_toxicity/bayes.py:57: RuntimeWarning: overflow encountered in exp\n",
      "  prob=1/(1+np.exp(log_Cscore-log_Tscore))\n"
     ]
    }
   ],
   "source": [
    "actual=df_train['toxic'].values\n",
    "actual_dev=df_dev['toxic'].values\n",
    "msk=actual\n",
    "Xtox = X_train_counts[msk,:]\n",
    "df_tox=df_train[msk]\n",
    "pred,prob,logT,logC,log_Tword,log_Cword=naive_bayes(X_train_counts,pw_tox,pw_cln,ptox)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbb72676d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Plot a histogram of the log probabilities.  \n",
    "plt.figure()\n",
    "plt.hist(np.maximum(-50,np.log(prob)),bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbaab935f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a histogram of the log-odds \n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "bins=np.linspace(-1000,1000,100)\n",
    "plt.hist(logT-logC,bins=bins,log=True)\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Log Odds of Toxicity')\n",
    "plt.title('Full Range')\n",
    "plt.subplot(122)\n",
    "bins=np.linspace(-20,20,100)\n",
    "plt.hist(logT-logC,bins=bins,log=True)\n",
    "plt.xlabel('Log Odds of Toxicity')\n",
    "plt.title('Zoomed in')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Maybe should also plot length of comments? To what extent are these mirroring a similar underlying shape, with long tails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbb7384940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "com_len=df_train['comment_clean'].apply(len)\n",
    "plt.hist(com_len,log=True)\n",
    "plt.xlabel('Character length of message')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4e3d3c2a1732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore_rates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jonathan/Data-Science/PDSG_DataScience/PDX_toxicity/util.py\u001b[0m in \u001b[0;36mcheck_predictions\u001b[0;34m(pred, actual, epsilon)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m# pred=np.reshape(pred,(len(actual),1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m#print(pred.shape,actual.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logloss,score_rates=check_predictions(pred,actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Interesting. The mean log-loss is surprisingly sensitive to the chosen zero-offset.  I think this reflects the fact that the naive-bayes method is returning a lot of incredibly small probabilities (10^{-100})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB(alpha=0.01)\n",
    "nb.fit(X_train_counts,df_train['toxic'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Make predictions on training/dev sets\n",
    "pred_nb=nb.predict(X_train_counts)\n",
    "pred_dev_nb=nb.predict(X_dev_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Training data\n",
      "True Positive 0.08531301672352806. False Positive 0.01651422232454947\n",
      "False Negative 0.011375766582246687. True Negative 0.8867969943696757\n",
      "Log-loss is 0.9632992952381051\n",
      "AUROC is 0.9320323498876192\n"
     ]
    }
   ],
   "source": [
    "print('Checking Training data')\n",
    "nb_stats=check_predictions(pred_nb,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dev set\n",
      "True Positive 0.06311753888352087. False Positive 0.02322102047813484\n",
      "False Negative 0.032478259514384565. True Negative 0.8811831811239598\n",
      "Log-loss is 1.9238035444874515\n",
      "AUROC is 0.8172894153987111\n"
     ]
    }
   ],
   "source": [
    "#Dev_set including 1-grams.  Evidently this is enough to overfit \n",
    "# the data, since performance is much worse than training set.\n",
    "print('Checking dev set')\n",
    "nb_stats=check_predictions(pred_dev_nb,actual_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Evidently including the 2-grams is massively overfitting the data.\n",
    "(It's essentially learning the repeated combinations that show up most in spammed toxic comments,\n",
    " and are thus only good for identifying those comments as toxic.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Naive Bayes False Positives and Negatives\n",
    "\n",
    "Let's now look a bit at the misclassified results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#fixing shapes to avoid broadcasting\n",
    "actual=np.reshape(actual,(len(actual),1))\n",
    "pred=np.reshape(pred,(len(actual),1))    \n",
    "\n",
    "fp_msk = ((pred==True)&(actual==False))    \n",
    "fn_msk = ((pred==False)&(actual==True))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_fn=df_train[fn_msk][['comment_clean','mean_toxic','median_toxic','toxic']]\n",
    "df_fp=df_train[fp_msk][['comment_clean','mean_toxic','median_toxic','toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                          comment_clean  \\\n128                    if i pick enough holes in you will you turn into swiss cheese ed   \n534          hey rich wuzzup my mom loves you so have fun huh thank you and good night    \n642               wow youre so clever so smooth stop being an ass so we can compromise    \n1136   i kant believe how sad everybody who writes things on this sight are get a life    \n1679                youre blocked from spamming do it again and youll be blocked again    \n\n      mean_toxic  median_toxic  toxic  \n128         -0.4           0.0  False  \n534          0.3           0.5  False  \n642         -0.4          -0.5  False  \n1136        -0.2           0.0  False  \n1679         0.1           0.0  False  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ind=df_fn.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                             comment_clean  \\\n7472    shut up you liar why arent you abiding by wikipedia policies that content must be based on veri...   \n8761                                         thanks thanks for participating in the conspiracy against me    \n14410  welcome faggot welcome hello and welcome to wikipedia thank you for your contributions i hope yo...   \n33567                                                                                   you i despise you    \n34109                                                                               put it up your bottom    \n\n       mean_toxic  median_toxic  toxic  \n7472    -0.800000          -1.0   True  \n8761    -0.400000          -1.0   True  \n14410   -0.500000          -1.0   True  \n33567   -0.900000          -1.0   True  \n34109   -0.777778          -1.0   True  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So false negatives.  Much more spacing/characters being used to avoid the filter.\n",
    "The false negatives in the larger set seem to be more rules-lawyering, whinging about admnistration, and sidestepping filters. e.g. f:)u:)c:)k:).\n",
    "This is a bit harder for the classifier to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So at least the \"false positives\" are because the people using the rating scale are wildly inconsistent.  These are \"-1\" on the toxicity scale, and so \"non-toxic\" under the rule where toxic comments have median toxicity less than -1.\n",
    "Some are \"neutral\" but have lots of repitition.  I can't for the life of me imagine any of these comments adding anything to the discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Let's use the truncated SVD for dimensionality reduction (or latent semantic analysis?)\n",
    "Apparently TF-IDF matrix is superior to straight term frequency matrix for this purpose  (more closely matches assumptions in the SVD about the noise.)\n",
    "Should maybe also symmetrize transformation (as suggested in paper comparing hyperparameters between word2vec and older SVD methods).\n",
    "They suggest using $T=U \\Lambda V = (U \\Lambda^{1/2}) (\\Lambda^{1/2} V)$ for the projection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=20,\n       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "#took a minute or two\n",
    "TSVD=TruncatedSVD(n_components=100,n_iter=20)\n",
    "TSVD.fit(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#actually transform the results \n",
    "X_train_trans=TSVD.transform(X_train_tfidf)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbaa8948d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbaee658d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(TSVD.explained_variance_)\n",
    "plt.xlabel('Singular value label')\n",
    "plt.ylabel('Singular value')\n",
    "plt.show()\n",
    "plt.plot(TSVD.explained_variance_)\n",
    "plt.xlabel('Singular value label')\n",
    "plt.ylabel('Singular value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So looks like power-law decay in the spectrum. We're primarily interested in using this\n",
    "for dimensionality reduction.  Ideally, I'd pick some reasonable threshold for keeping a\n",
    "certain fraction of the explained variance.\n",
    "(Could estimate a power law tail, compute threshold to capture that percentage).\n",
    "\n",
    "But for now, we'll just set the threshold to be 100, as a suitably small arbitrary choice.  \n",
    "We will next use the transformed results in a \"deep\" neural network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#actually transform the dev/test data.\n",
    "X_dev_trans=TSVD.transform(X_dev_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Nsub=1000\n",
    "np.random.seed(454)\n",
    "#Should really update to just use sklearn's stratified Kfold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Deep Network\n",
    "\n",
    "Another idea is to build a deep neural network on the term-frequency matrix, effectively running with extensions to the Naive Bayes model.\n",
    "This will use the reduced term-frequency matrix after the Truncated SVD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, l2_regularizer\n",
    "from tensorflow.contrib.rnn import BasicRNNCell,LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from deep_network import deep_dropout_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb80f7dd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter #1000. Current log-loss:0.13786394894123077\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing regularization_losses.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'function' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "#Ignore the \"error about serializing - this is a known problem with saving models created using \n",
    "#modules like fully connected, since their components are not named.\n",
    "#The models are saved, and the computations work.\n",
    "\n",
    "actual=df_train['toxic'].astype(int).values\n",
    "save_name='./tf_models/deep_relu_drop'\n",
    "dNN=deep_dropout_NN(X_train_trans.shape)\n",
    "dNN.run_graph(X_train_trans,actual,save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newer predict\n",
      "INFO:tensorflow:Restoring parameters from ./tf_models/deep_relu_drop-1000\n"
     ]
    }
   ],
   "source": [
    "#model_name='tf_models/deep_relu_drop-{}'.format(dNN.n_iter)\n",
    "model_name='./tf_models/deep_relu_drop-{}'.format(1000)\n",
    "dnn_pred2=dNN.predict_all(model_name,X_train_trans)\n",
    "dnn_pred2=dnn_pred2.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive 0.06058354438328066. False Positive 0.006645457019067752\n",
      "False Negative 0.036105238922494086. True Negative 0.8966657596751575\n",
      "Log-loss is 1.4765620415427758\n",
      "AUROC is 0.8096130944597727\n"
     ]
    }
   ],
   "source": [
    "#Check scores on training data\n",
    "dnn_conf=check_predictions(dnn_pred2,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive 0.08531301672352806. False Positive 0.01651422232454947\n",
      "False Negative 0.011375766582246687. True Negative 0.8867969943696757\n",
      "Log-loss is 0.9632992952381051\n",
      "AUROC is 0.9320323498876192\n"
     ]
    }
   ],
   "source": [
    "#Compare with naive-bayes training\n",
    "nb_conf=check_predictions(pred_nb,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Right now the network seems undertrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Predictions from Deep Neural Network\n",
    "\n",
    "Let's now run some predictions on the full training and development sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer ReLU network: on Dev set\n",
      "True Positive 0.05386029984727114. False Positive 0.011594925661565315\n",
      "False Negative 0.04173549855063429. True Negative 0.8928092759405293\n",
      "Log-loss is 1.841976868183656\n",
      "AUROC is 0.7752982535343149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newer predict\n",
      "INFO:tensorflow:Restoring parameters from tf_models/deep_relu_drop-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer ReLU network: on Dev set\n",
      "True Positive 0.06067773196307847. False Positive 0.006614061159135149\n",
      "False Negative 0.036011051342696276. True Negative 0.8966971555350901\n",
      "Log-loss is 1.4722245180949745\n",
      "AUROC is 0.8101175383672511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newer predict\n",
      "INFO:tensorflow:Restoring parameters from tf_models/deep_relu_drop-1000\n"
     ]
    }
   ],
   "source": [
    "#Try testing on the dev-set\n",
    "model_name='tf_models/deep_relu_drop-{}'.format(dNN.n_iter)\n",
    "nn_pred_train = dNN.predict_all(model_name,X_train_trans)\n",
    "print('4 layer ReLU network: on Dev set')\n",
    "actual_train=df_train['toxic'].values\n",
    "nn_pred_train=nn_pred_train.reshape(-1)\n",
    "nn_stats=check_predictions(nn_pred_train,actual_train)\n",
    "\n",
    "nn_pred_dev = dNN.predict_all(model_name,X_dev_trans)\n",
    "print('4 layer ReLU network: on Dev set')\n",
    "actual_dev=df_dev['toxic'].values\n",
    "nn_pred_dev=nn_pred_dev.reshape(-1)\n",
    "nn_stats=check_predictions(nn_pred_dev,actual_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes on Dev set\n",
      "True Positive 0.06311753888352087. False Positive 0.02322102047813484\n",
      "False Negative 0.032478259514384565. True Negative 0.8811831811239598\n",
      "Log-loss is 1.9238035444874515\n",
      "AUROC is 0.8172894153987111\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes on Dev set')\n",
    "pred_dev_nb=nb.predict(X_dev_counts)\n",
    "nb_stats=check_predictions(pred_dev_nb,actual_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Prior work with a (3-layer ReLU-tanh) network led to over-fitting to the training set.  Not surprising, since there was no regularization here.\n",
    "It could outperform the Naive Bayes method on the training set, but had worse performance on the development dataset.\n",
    "\n",
    "Let's put in some dropout. Putting in dropout after each layer, with a 0.1 dropout probability improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.63848495096381463, 0.69363963655066008]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dev scores\n",
    "[f1_score(actual_dev,nn_pred_dev),f1_score(actual_dev,pred_dev_nb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75269211943220748, 0.97498410006359981]\n",
      "[0.68780126065999259, 0.66262049268118539]\n"
     ]
    }
   ],
   "source": [
    "print([f1_score(actual,nn_pred_train),f1_score(actual,pred_nb)])\n",
    "print([f1_score(actual_dev,nn_pred_dev),f1_score(actual_dev,pred_dev_nb)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "I am finding that beyond one or two layers, the network just seems to output zeros.  Maybe the learning rate was too high? Yes - this is a common problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "So let's try the current flavour of the month approach: a recurrent neural network.\n",
    "Based on talking to Joseph and Fahim at the group, they used a two-layer neural network based on the just the 2000 most common words, using ReLU activation.  (I think they said their approach was inspired by someone at Kaggle.)\n",
    "Let's try something similar, with initially a single layer leaky ReLU layer.\n",
    "\n",
    "The idea is that the network parses each word of the sentence (to better capture logical structure).\n",
    "Each word needs an index.  Initially this is an index in the vocabulary V, where $V\\sim10^6$ or more.  That's an infeasibly large matrix.\n",
    "We need some form of dimensionality reduction.  Either by picking the most distinctive words (which actually appear in multiple messages),\n",
    "or by projecting down via SVD. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "init_explore.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
