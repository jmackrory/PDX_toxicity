{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Toxicity in Wikipedia Comments\n",
    "\n",
    "This is a parallel work to any work on the wikipedia toxicity data on the same topic.\n",
    "This data has not been cleaned yet, and has not had multiple categories for the variety of toxity introduced yet.\n",
    "\n",
    "Beware: Lots of swearing, racism, homophobia, misogyny is contained within due to nature of the comments.\n",
    "And the fact I have searched for nasty terms as a sanity check on how the methods are working.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Looking at the comments data, we'll need to clean the data quite a bit (lots of newlines, weird characters).\n",
    "There is also a lot of wikipedia markup, and mis-spelled words.\n",
    "\n",
    "My rough plan is to build up a lexicon, tokenize that data, and try to build a Naive Bayes model.  (Maybe later a Recurrent Neural network model?)\n",
    "\n",
    "Cleaning:\n",
    "* Clean data : How to remove newlines (search/replace: NEWLINE with '')  (done)\n",
    "* Tokenize (convert words to indices) (done)\n",
    "* Stemming words\n",
    "* Balancing data set\n",
    "* Match up comments, and review scores (done)\n",
    "* Search for gibberish words (make a new \"feature\" for badly spelled comments)\n",
    "\n",
    "Embeddings:\n",
    "These are necessary to reduce the dimensionality of the problem to a scale that will fit in memory.  \n",
    "   * SVD - use SVD on the term-frequency matrix. Will use truncated SVD.  \n",
    "   * word2vec - train vectors for words based on surrounding contexts (can use pre-trained ones)\n",
    "   * Latent Factor Analysis - maybe useful prelude or alternative for building up embeddings.\n",
    "                            - ALS is similar to SVD, but not guaranteed to be orthogonal.\n",
    "   * Keep only most common words (in both toxic/non-toxic), or highest probability of toxic/non-toxic\n",
    "\n",
    "Other Analysis possibilities:\n",
    "* Naive Bayes\n",
    "    - can find most important words\n",
    "    - simple, easy to understand baseline.\n",
    "* Support Vector Machine\n",
    "    - try ensemble method (split the data into batches, and train an SVM on each batch.  Then do a committee vote.)\n",
    "      This turns O(n_sample^3) scaling into O(n_sample^3/n_batch^2) scaling on the training.\n",
    "      This is effectively treating the kernel matrix as if it were block-diagonal, as it omits correlations between datasets.\n",
    "      Perhaps running multiple copies with different random splits would work?\n",
    "* Deep Neural Network\n",
    "    - Build a network using the term-frequency matrix as inputs.\n",
    "    - Extends the naive Bayes method.  (Might be automatic way of doing some of that SVM stuff?)\n",
    "    - Employ dropout for regularization, alongside L2 penalties.  \n",
    "     \n",
    "* Recurrent Neural Network\n",
    "    - Build up word embeddings (word2vec), or just use the pretrained ones.\n",
    "    - This one runs at the sentence/paragraph level and keeps the temporal structure.\n",
    "    - Use LSTM/GRU cells, with a couple layers. \n",
    "    - Also dropout, l2 penalties\n",
    "\n",
    "Metrics:\n",
    "    - F1 :harmonic mean of precision and recall\n",
    "    - log-loss $N^{-1}\\sum_{j=1}^N\\sum_c y_{jc}\\log \\hat{y}_{jc}$, where $j$ runs over observations, and $c$ runs over classes.\n",
    "    - AUROC: Something like Gini coefficient?  (Plot the true-positive/false-positive curve as the decision threshold $t$ is varied.)\n",
    "The last two were used as Kaggle metrics.  They just changed over to the column average AUC-ROC metric.  Apparently this is less sensitive to leader-board climbing than the log-loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import log_loss,f1_score,roc_auc_score\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "#my code\n",
    "from bayes import cond_prob, naive_bayes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159686, 7) (1598289, 4)\n"
     ]
    }
   ],
   "source": [
    "#df_com = pd.read_csv('data/toxicity_annotated_comments_unanimous.tsv',sep='\\t')\n",
    "#df_rate = pd.read_csv('data/toxicity_annotations_unanimous.tsv',sep='\\t')\n",
    "df_com = pd.read_csv('data/toxicity_annotated_comments.tsv',sep='\\t')\n",
    "df_rate = pd.read_csv('data/toxicity_annotations.tsv',sep='\\t')\n",
    "\n",
    "#make rev_id an integer\n",
    "df_com['rev_id']=df_com['rev_id'].astype(int)\n",
    "df_rate['rev_id']=df_rate['rev_id'].astype(int)\n",
    "\n",
    "print(df_com.shape, df_rate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rev_id', 'comment', 'year', 'logged_in', 'ns', 'sample', 'split',\n",
       "       'scores', 'mean_toxic', 'median_toxic', 'toxic', 'comment_clean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_com.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbb7b64be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#When are the comments made?\n",
    "plt.figure()\n",
    "bin_arr=np.sort(df_com['year'].unique())\n",
    "df_com['year'].hist(bins=bin_arr)\n",
    "plt.title('Total Comments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#make a new column in df_com with array of worker_ids, and toxicity\n",
    "df_com['scores']=None\n",
    "\n",
    "#since 'rev_id' is sorted, can take first difference, and find where\n",
    "#there are changes in 'rev_id'.  Those set the boundaries for changes.\n",
    "change_indices=df_rate.index[df_rate['rev_id'].diff()!=0].values\n",
    "\n",
    "#use numpy split to split the array into many sub-arrays.\n",
    "arr=df_rate[['worker_id','toxicity_score']].values\n",
    "split_arr=np.split(arr,change_indices)\n",
    "#drop first index as empty\n",
    "split_arr.pop(0)\n",
    "df_com['scores']=split_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def score_mean(score_list):\n",
    "    \"\"\"score_mean\n",
    "    Compute mean of toxicity scores for input array.\n",
    "    Array is first (and only) element in the input list.\n",
    "    Compute mean running down the rows.  Could be updated to include weighted sum of weights\n",
    "    \"\"\"\n",
    "    s = np.mean(score_list[:,1])\n",
    "    return s\n",
    "\n",
    "def score_median(score_list):\n",
    "    \"\"\"score_median\n",
    "    Compute median of toxicity scores for input array.\n",
    "    Array is first (and only) element in the input list.\n",
    "    Compute mean running down the rows.  Could be updated to include weighted sum of weights\n",
    "    \"\"\"\n",
    "    s = np.median(score_list[:,1])\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Make a new column computing mean, median scores\n",
    "df_com['mean_toxic']=df_com['scores'].apply(score_mean)\n",
    "df_com['median_toxic']=df_com['scores'].apply(score_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#So 373 duplicated comments.  Awesome.  \n",
    "#dup_msk=df_com['comment'].duplicated(keep=False)\n",
    "#I'm just going to drop these duplicates\n",
    "df_com.drop_duplicates(subset='comment',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments: 159463. Toxic comments: 15353. Toxic Fraction: 0.09627938769495117\n"
     ]
    }
   ],
   "source": [
    "#Define toxic comments as those where the median is below -1, or -2.\n",
    "#-1 captures more comments, but with more variance in what is considered toxic/unhelpful.\n",
    "df_com['toxic']=(df_com['median_toxic']<=-1)\n",
    "Ntoxic=df_com['toxic'].sum()\n",
    "Ntot=len(df_com)\n",
    "print(\"Total comments: {}. Toxic comments: {}. Toxic Fraction: {}\".format(Ntot,Ntoxic,Ntoxic/Ntot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f10d074eeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#When are the comments made?  Has the toxicity changed over time?\n",
    "#Note this is on the full dataset, with test/training/dev splits. \n",
    "plt.figure()\n",
    "bin_arr=np.sort(df_com['year'].unique())\n",
    "#non-toxic comments\n",
    "plt.subplot(2,2,1)\n",
    "msk1=df_com['median_toxic']<=-1\n",
    "plt.ylabel('Toxicity=-1')\n",
    "df_com['year'][msk1].hist(bins=bin_arr)\n",
    "plt.title('Toxic')\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Non-Toxic')\n",
    "df_com['year'][~msk1].hist(bins=bin_arr)\n",
    "#second row\n",
    "plt.subplot(2,2,3)\n",
    "msk2=df_com['median_toxic']<=-2\n",
    "df_com['year'][msk2].hist(bins=bin_arr)\n",
    "plt.ylabel('Toxicity=-2')\n",
    "plt.subplot(2,2,4)\n",
    "df_com['year'][~msk2].hist(bins=bin_arr)\n",
    "plt.xlabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So the data looks to be evenly balanced as toxic/non-toxic across time, with a rough 10% fraction reduction from regular to toxic, to severely toxic.\n",
    "Another question about the data is what topics were under discussion? Does this bias the output/findings? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#cleaning the data\n",
    "#Can use pandas built in str functionality with regex to eliminate\n",
    "#Can maybe also eliminate all punctuation?  Makes any \n",
    "\n",
    "#maybe also dates?\n",
    "def clean_up(comments):\n",
    "    com_clean=comments.str.replace('NEWLINE_TOKEN',' ')\n",
    "    com_clean=com_clean.str.replace('TAB_TOKEN',' ')    \n",
    "    #Remove HTML trash, via non-greedy replacing anything between backticks.\n",
    "    #Should probably combine into a single regex.\n",
    "    #re_str=\"(style|class|width|align|cellpadding|cellspacing|rowspan|colspan)=\\`\\`.*?\\`\\`\"\n",
    "    #com_clean=com_clean.str.replace(re_str,' ')\n",
    "    com_clean=com_clean.str.replace(\"style=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"class=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"width=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"align=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"cellpadding=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"cellspacing=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"rowspan=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"colspan=\\`\\`.*?\\`\\`\",' ')\n",
    "    #remove numbers\n",
    "    #com_clean=com_clean.str.replace(\"[0-9]+\",' ')\n",
    "    #remove numbers\n",
    "    com_clean=com_clean.str.replace(\"_\",' ')\n",
    "    #remove symbols.    There must be a more comprehensive way of doing this?\n",
    "    com_clean=com_clean.str.replace(\"[\\[\\[\\{\\}=_:\\|\\(\\)\\\\\\/\\`]+\",' ')\n",
    "    #remove multiple spaces, replace with a single space\n",
    "    com_clean=com_clean.str.replace('\\\\s+',' ')\n",
    "    return com_clean\n",
    "df_com['comment_clean']=clean_up(df_com['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This does lose some information.  Such as possible rude symbols replicating breasts, or genitalia. (There's like 6 of these crude ascii art drawings.  This is probably not worth tracking down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rev_id', 'comment', 'year', 'logged_in', 'ns', 'sample', 'split',\n",
       "       'scores', 'mean_toxic', 'median_toxic', 'toxic', 'comment_clean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_com.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#put the file as tsv \n",
    "df_com.to_csv('saved_dataframes/cleaned_comments.tsv.gzip',sep='\\t',\n",
    "columns=['rev_id','comment_clean','scores','mean_toxic','median_toxic','split','toxic'],compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#read in saved cleaned up dataframe.\n",
    "df_com=pd.read_csv('saved_dataframes/cleaned_comments.tsv.gzip',sep='\\t',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#separate off training_split\n",
    "train_msk=df_com['split']=='train'\n",
    "df_train=df_com[train_msk]\n",
    "df_dev=df_com[df_com['split']=='dev']\n",
    "df_test=df_com[df_com['split']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95554, 133822)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#borrowing from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "count_vect=CountVectorizer(stop_words='english',lowercase=True,strip_accents='unicode')\n",
    "tfidf_vect=TfidfVectorizer(stop_words='english',lowercase=True,strip_accents='unicode')\n",
    "X_train_counts=count_vect.fit_transform(df_train['comment_clean'])\n",
    "X_train_tfidf=tfidf_vect.fit_transform(df_train['comment_clean'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#do the same transformations using existing vocab built up in training.\n",
    "X_dev_tfidf=tfidf_vect.transform(df_dev['comment_clean'])\n",
    "X_test_tfidf=tfidf_vect.transform(df_test['comment_clean'])\n",
    "\n",
    "X_dev_counts=count_vect.transform(df_dev['comment_clean'])\n",
    "X_test_counts=count_vect.transform(df_test['comment_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<32083x133822 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 778855 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Checking the vectorizer and finding common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "I wanted to check that the vectorizer was working by outputting common words, and identifying the \"most toxic\" words, based on their counts.\n",
    "This was useful as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#get vocabulary dictionary\n",
    "voc_dict=count_vect.vocabulary_\n",
    "#make a dataframe, with entries as rows\n",
    "voc_df=pd.DataFrame.from_dict(voc_dict,orient='index')\n",
    "#sort by row entry value, and then use that as the index for the counts.\n",
    "voc_df1=voc_df.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29143\n",
       "Name: dick, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_df1.iloc[29143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ptox,pw_tox,pw_cln = cond_prob( X_train_counts, df_train['toxic'].values, csmooth=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voc_df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-41fb565b9ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_cond\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpw_tox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mptox\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpw_tox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mptox\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpw_cln\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mptox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword_mat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_cond\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpw_cln\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpw_tox\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mword_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pcond'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'p_clean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'p_toxic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvoc_df1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mword_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pcond'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpcond_wds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'voc_df1' is not defined"
     ]
    }
   ],
   "source": [
    "#make new dataframe with conditional probabilities for words being toxic, and raw probabilities of occuring in toxic/clean messages\n",
    "X_cond= pw_tox*ptox/(pw_tox*ptox + pw_cln*(1-ptox))\n",
    "word_mat=np.array([X_train_counts.sum(axis=0),X_cond,pw_cln,pw_tox]).squeeze()\n",
    "word_df=pd.DataFrame(word_mat.T,columns=['count','pcond','p_clean','p_toxic'],index=voc_df1.index)\n",
    "word_df.sort_values('pcond',ascending=False,inplace=True)\n",
    "pcond_wds=word_df.head(n=20).index.values\n",
    "print(pcond_wds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So, the most toxic words (i.e. words that only appeared in toxic messages) are misspelled attempts at rudeness, with weird spaces, and combination words.  I think this reflects more on the pre-processing.  These words show up in a single toxic message, and are thus great at inferring that one message is toxic.  This doesn't say much about more general trends in the messages.\n",
    "\n",
    "I am considering also implementing a spell-check, and adding a variable for the number of incorrect words or fraction of the message that is misspelled.  Another feature would be the fraction that is capitalized?\n",
    "The accent stripping catches simple attempts to avoid the spam filter with accents, but does miss things where the words are spaced out, or have other characters inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "xtot=X_train_counts.sum(axis=0).squeeze()\n",
    "#compare vectorized vs. naive counts to check mappings\n",
    "def check_vect(count_mat,comments,vocab,word):\n",
    "    \"\"\"check_vect(count_mat,comments,vocab,word)\n",
    "    Checks the counts/occurence of words between the count vectorizer,\n",
    "    and a naive 'contains' search.  Returns all the matching comments,\n",
    "    and any discrepencies.        \n",
    "    \"\"\"\n",
    "    ind=vocab.loc[word].values\n",
    "    xtot=count_mat.sum(axis=0)\n",
    "    vect_count=(xtot[0,ind])\n",
    "    #find comments with words\n",
    "    msk=(count_mat[:,ind]>0).toarray().squeeze()\n",
    "    #find comments via naive search\n",
    "    naive_msk=comments.str.contains('{}'.format(word),case=False)\n",
    "    naive_count=np.sum(naive_msk)\n",
    "    comments=comments[msk]\n",
    "    naive_comments=comments[naive_msk]\n",
    "    diff_comments=comments[msk!=naive_msk]\n",
    "    return vect_count,naive_count,comments,naive_comments,diff_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vc,cc,com,ncom,dcom=check_vect(X_dev_counts,df_dev['comment'],voc_df,pcond_wds[0])\n",
    "#searching for 'fuck' gives a salutory lesson in why accent tripping is worthwhile, and a simple word filter will probably be circumvented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fucksex'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcond_wds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vect: [[0]], Naive: 0\n",
      "Series([], Name: comment, dtype: object) \n",
      "\n",
      "\n",
      "Series([], Name: comment, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "#currently searching for \"gay\", a term that has clean connotations, but can be used in homophobic attacks.\n",
    "#Another word with the same dichotomy of identity/hate is Jew. Or Muslim.  \n",
    "print('Vect: {}, Naive: {}'.format(vc,cc))\n",
    "print(com.head(),'\\n\\n')\n",
    "print(ncom.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuntbag :\t 128 occurences: \t 3 messages\n",
      "cuntliz :\t 111 occurences: \t 1 messages\n",
      "yourselfgo :\t 309 occurences: \t 1 messages\n",
      "marcolfuck :\t 260 occurences: \t 1 messages\n",
      "fack :\t 232 occurences: \t 2 messages\n",
      "veggietales :\t 212 occurences: \t 1 messages\n",
      "ancestryfuck :\t 208 occurences: \t 1 messages\n",
      "notrhbysouthbanof :\t 208 occurences: \t 2 messages\n",
      "shitfuck :\t 182 occurences: \t 1 messages\n",
      "yaaaa :\t 128 occurences: \t 1 messages\n",
      "haahhahahah :\t 128 occurences: \t 1 messages\n",
      "fucksex :\t 624 occurences: \t 1 messages\n",
      "buttsecks :\t 498 occurences: \t 2 messages\n",
      "bastered :\t 449 occurences: \t 2 messages\n",
      "cocksucker :\t 425 occurences: \t 37 messages\n",
      "fggt :\t 398 occurences: \t 5 messages\n",
      "mothjer :\t 391 occurences: \t 4 messages\n",
      "offfuck :\t 360 occurences: \t 1 messages\n",
      "niggas :\t 340 occurences: \t 7 messages\n",
      "sexsex :\t 332 occurences: \t 1 messages\n"
     ]
    }
   ],
   "source": [
    "#naughty_word=['fuck','fag','kill','bleach','bellend','wanker','towelhead']\n",
    "#identity_hate=['nigger','trans','faggot','kike','jew','wetback','spic']\n",
    "word_counts=X_train_counts.sum(axis=0)\n",
    "for word in pcond_wds:\n",
    "    try:\n",
    "        ind=count_vect.vocabulary_[word]\n",
    "        n_occur=word_counts[0,ind]\n",
    "        n_tot=np.sum(X_train_counts[:,ind]>0)\n",
    "        print(word,':\\t {} occurences: \\t {} messages'.format(n_occur,n_tot))\n",
    "    except:\n",
    "        print(word,'not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So, the conditional probability for a word being toxic is chiefly determined by whether it only occurs in toxic messages.  In this case, these \"most toxic words\" are misspelled or portmanteus.  The high counts are offset by only appearing in few messages.  This suggest these are just lengthy repetitions or a single rude message.\n",
    "\n",
    "A spellcheck might catch these, and correct the spelling?  That would potentially catch the attempts to circumvent obvious mis-spelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "I noticed that there are very few obvious racist slurs in the unanimous data set. (lots of sexism, general hate)\n",
    "Weird sociological question on perception of toxicity of racism, perhaps by american reviewers? (this is something that the actual original project is explicitly considering at https://conversationai.github.io/bias.html)\n",
    "\n",
    "(searching for the n-word found these)\n",
    "Some ratings seem way off. e.g. the scores for comments 1467, 1657 include some -1s.\n",
    "Someone even thought 1918 was neutral!\n",
    "Wait, 2669 and 2670 are now identical comments. And some raters thought that 2670 was neutral too!  What the hell?!\n",
    "This suggests using the median toxicity score to avoid the mean being contaminated by people with a really different sense "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Naive Bayes\n",
    "\n",
    "I want to implement a Naive Bayes classifier as a baseline.  I've written my own version, which I will try to compare to\n",
    "scikit-learn's version.  (They both return the same result now).\n",
    "\n",
    "This basically treats the comments in a bag-of-words sense, and drops any correlations between the words.  Perhaps including some more\n",
    "common n-grams, e.g. \"frigging crank\".\n",
    "\n",
    "* Estimate $p(w|T)$ from counts in term-frequency matrix.\n",
    "* Use Bayes Rule\n",
    "  $ P(T|w) = \\frac{p(T)p(w|T)}{\\text{normalization const}}\n",
    "\n",
    "  \\begin{equation}\n",
    "    p(T|\\text{words}) = P(T) \\prod_{words}\\frac{p(w_i|T)}{p(w_i|T)\n",
    "  \\end{equation}\n",
    "\n",
    "* Use Logarithms, and compare log-odds for toxicity/non-toxic.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/Data-Science/PDX_DataScience/PDX_toxicity/bayes.py:57: RuntimeWarning: overflow encountered in exp\n",
      "  prob=1/(1+np.exp(log_Cscore-log_Tscore))\n"
     ]
    }
   ],
   "source": [
    "actual=df_train['toxic'].values\n",
    "msk=actual\n",
    "Xtox = X_train_counts[msk,:]\n",
    "df_tox=df_train[msk]\n",
    "pred,prob,logT,logC,log_Tword,log_Cword=naive_bayes(X_train_counts,pw_tox,pw_cln,ptox)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbaf4abdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a histogram of the log probabilities.  \n",
    "plt.figure()\n",
    "plt.hist(np.maximum(-50,np.log(prob)),bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb553aa898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a histogram of the log-odds (right term?).  \n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "bins=np.linspace(-1000,1000,100)\n",
    "plt.hist(logT-logC,bins=bins,log=True)\n",
    "plt.subplot(122)\n",
    "bins=np.linspace(-20,20,100)\n",
    "plt.hist(logT-logC,bins=bins,log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Maybe should also plot length of comments? To what extent are these mirroring a similar underlying shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95554, 1) (95554, 1)\n",
      "True Positive 0.08529208615023966. False Positive 0.016461895891328463\n",
      "False Negative 0.01139669715553509. True Negative 0.8868493208028968\n",
      "Log-loss is 0.9622148788120853\n",
      "AUROC is 0.931953076744998\n"
     ]
    }
   ],
   "source": [
    "def check_predictions(pred,actual,epsilon=1E-15):\n",
    "    \"\"\"check_predictions\n",
    "    Compares predicted class (y_i) against actual class (z_i).\n",
    "    Returns the confusion matrix and mean log-loss.\n",
    "    \n",
    "    Log-loss = sum_i{ z_i log[ y_i] }/M\n",
    "\n",
    "    Input: pred - predicted values (0,1)\n",
    "    actual - true labels \n",
    "    eps    - shift to avoid log(0)\n",
    "    Returns: Confusion matrix with [[true positive, false positive],[false negative, true negative]]\n",
    "    log-loss - average log-loss\n",
    "    \"\"\"\n",
    "    actual=np.reshape(actual,(len(actual),1))\n",
    "    pred=np.reshape(pred,(len(actual),1))    \n",
    "    print(pred.shape,actual.shape)\n",
    "    tp = np.mean((pred==True)&(actual==True))\n",
    "    tn = np.mean((pred==False)&(actual==False))\n",
    "    fp = np.mean((pred==True)&(actual==False))    \n",
    "    fn = np.mean((pred==False)&(actual==True))            \n",
    "    scores=np.matrix([[tp,fp],[fn,tn]])\n",
    "    print(\"True Positive {}. False Positive {}\".format(tp,fp))\n",
    "    print(\"False Negative {}. True Negative {}\".format(fn,tn))\n",
    "    pred_num=pred.astype(float)\n",
    "    logloss=log_loss(actual,pred_num,eps=epsilon,normalize=True)    \n",
    "    #give zero a small correction.\n",
    "    #pred_num[pred==False]=epsilon\n",
    "    #pred_num[pred==True]=1-epsilon\n",
    "    #my (initial) wrong attempt\n",
    "    #logloss2=-np.mean(np.multiply(actual,np.log(pred_num)))\n",
    "    # logloss2=-np.mean(np.multiply(actual,np.log(pred_num))\\\n",
    "    #     +np.multiply(1-actual,np.log(1-pred_num)))\n",
    "    # print(logloss2)\n",
    "    auroc = roc_auc_score(actual,pred)\n",
    "    #logloss=0\n",
    "    print(\"Log-loss is {}\".format(logloss))\n",
    "    print(\"AUROC is {}\".format(auroc))    \n",
    "    return scores,logloss\n",
    "logloss,score_rates=check_predictions(pred,actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Interesting. The mean log-loss is surprisingly sensitive to the chosen zero-offset.  I think this reflects the fact that the naive-bayes method is returning a lot of incredibly small probabilities (10^{-100})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Look at the false negatives \n",
    "# df_fn=df_train[(pred==False)]\n",
    "# df_fn=df_fn[df_fn['toxic']==True]\n",
    "# df_fn[['comment_clean','mean_toxic','median_toxic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The false negatives in that larger seem to be more rules-lawyering, whinging about admnistration, and sidestepping filters. e.g. f:)u:)c:)k:).\n",
    "This is a bit harder for the classifier to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB(alpha=0.01)\n",
    "nb.fit(X_train_counts,df_train['toxic'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95554, 1) (95554, 1)\n",
      "True Positive 0.08529208615023966. False Positive 0.016461895891328463\n",
      "False Negative 0.01139669715553509. True Negative 0.8868493208028968\n",
      "Log-loss is 0.9622148788120853\n",
      "AUROC is 0.931953076744998\n"
     ]
    }
   ],
   "source": [
    "pred_nb=nb.predict(X_train_counts)\n",
    "nb_stats=check_predictions(pred_nb,actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Well, I must have an error in my Naive Bayes code.  I'm getting much worse results.  Have correct class probabilities, and conditional probabilities.  (Found the bug.  Used $\\prod_i[p(w_i|T)p(T)]$ instead of\n",
    "$p(T)\\prod_i p(w_i|T)$.  Found my error after examining hidden attributes, but really checking and copying from the sklearn source code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Naive Bayes False Positives and Negatives\n",
    "\n",
    "Let's now look a bit at the misclassified results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#fixing shapes to avoid broadcasting\n",
    "actual=np.reshape(actual,(len(actual),1))\n",
    "pred=np.reshape(pred,(len(actual),1))    \n",
    "\n",
    "fp_msk = ((pred==True)&(actual==False))    \n",
    "fn_msk = ((pred==False)&(actual==True))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_fn=df_train[fn_msk][['comment_clean','mean_toxic','median_toxic','toxic']]\n",
    "df_fp=df_train[fp_msk][['comment_clean','mean_toxic','median_toxic','toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                            comment_clean  \\\n128                                If I pick enough holes in you, will you turn into swiss cheese? ^ ^ Ed   \n534                     hey rich WUZZUP? My mom loves you... SO HAVE FUN HUH!!! THANK YOU AND GOOD NIGHT!   \n642                         ****Wow, you're so clever. So smooth. Stop being an ass so we can compromise!   \n1136                   I.Kant believe how sad everybody who writes things on this sight are- get a life!    \n1767   Just a question... Talk about Lir, not to them How do we know that these ban timer reset edits ...   \n\n      mean_toxic  median_toxic  toxic  \n128         -0.4           0.0  False  \n534          0.3           0.5  False  \n642         -0.4          -0.5  False  \n1136        -0.2           0.0  False  \n1767         0.3           0.0  False  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ind=df_fn.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                            comment_clean  \\\n803        Are you trying to wind me up or something? You are a vandal undoing useful proofreading work.    \n924    Actually, it is you who resorted to pathetic insults first. Let me quote what you wrote remove ...   \n1354   I am not user 168.209.97.34. On what basis are you acusing me of being that user? Please answer...   \n1374                                              Make that four, fifth is underway. This is rediculous!    \n1736   No! This is a GROUP EFFORT! Wikipedia is a collaborative COMMUNITY and there are no school essa...   \n\n      mean_toxic  median_toxic  toxic  \n803         -0.8          -1.0   True  \n924         -0.6          -1.0   True  \n1354        -0.6          -1.0   True  \n1374        -0.6          -1.0   True  \n1736        -0.3          -1.0   True  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So false negatives.  Much more spacing/characters being used to avoid the filter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So at least the \"false positives\" are because the people using the rating scale are wildly inconsistent.  These are \"-1\" on the toxicity scale, and so \"non-toxic\" under the rule where toxic comments have median toxicity less than -1.\n",
    "Some are \"neutral\" but have lots of repitition.  I can't for the life of me imagine any of these comments adding anything to the discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Let's use the truncated SVD for dimensionality reduction (or latent semantic analysis?)\n",
    "Apparently TF-IDF matrix is superior to straight term frequency matrix for this purpose  (more closely matches assumptions in the SVD about the noise.)\n",
    "Should maybe also symmetrize transformation (as suggested in paper comparing hyperparameters between word2vec and older SVD methods).\n",
    "They suggest using $T=U \\Lambda V = (U \\Lambda^{1/2}) (\\Lambda^{1/2} V)$ for the projection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "?TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=10,\n       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#took a minute or two\n",
    "TSVD=TruncatedSVD(n_components=100,n_iter=10)\n",
    "TSVD.fit(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#actually transform the results \n",
    "X_train_trans=TSVD.transform(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `TSVD.transform` not found.\n"
     ]
    }
   ],
   "source": [
    "?TSVD.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbaadb2208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(TSVD.explained_variance_)\n",
    "plt.xlabel('Singular value label')\n",
    "plt.ylabel('Singular value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We will next use the transformed results in a \"deep\" neural network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#actually transform the dev/test data.\n",
    "X_dev_trans=TSVD.transform(X_dev_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "In CS229, Andrew Ng's assignment 2 suggest the SVM as a natural improvement over the Naive Bayes method.\n",
    "# Let's implement one of those.  I'm going to update it to do batch gradient descent with sparse matrices.\n",
    "# The version I wrote initially was trash, I am attempting to vectorize the code using appropriate scipy.sparse matrix operations.\n",
    "\n",
    "Or I could use an ensemble of SVM's based on subsets of the data. That leverages the existing (presumably smarter) scikit-learn code, in a way that could scale up.\n",
    "\n",
    "Or use an approximate kernel via Random Fourier Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "947"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual=df_train['toxic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Nsub=1000\n",
    "np.random.seed(454)\n",
    "def get_subset(frac_perc,dat_mat,labels):\n",
    "    \"\"\"get_subset\n",
    "    Returns random subset of the data and labels.\n",
    "    Maintains same fraction of toxic/non-toxic data as the full dataset.\n",
    "    \"\"\" \n",
    "    #make vector and sample indices for true/false.\n",
    "    nvec=np.arange(len(labels))\n",
    "    #get the indices for true/false\n",
    "    Tvec=nvec[labels]\n",
    "    Cvec=nvec[~labels]\n",
    "    #grab a random shuffling of those indices.\n",
    "    np.random.shuffle(Tvec)\n",
    "    np.random.shuffle(Cvec)\n",
    "    #grab some fraction of them.\n",
    "    it = int(len(Tvec)*frac_perc)\n",
    "    ic = int(len(Cvec)*frac_perc)\n",
    "    ind_sub=np.append(Tvec[:it],Cvec[:ic])\n",
    "    Xsub = dat_mat[ind_sub]\n",
    "    label_sub = labels[ind_sub].reshape((len(ind_sub),1))\n",
    "    return ind_sub,Xsub,label_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "ind_sub,Xsub,label_sub=get_subset(0.01,X_train_counts,actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 of 382\n",
      "Iter 191 of 382\n"
     ]
    }
   ],
   "source": [
    "#my code: super slow.\n",
    "#TODO: Look into Cython.  Does it play nice with sparse?\n",
    "#Just use scikit-learns SVM, and approximate kernels.\n",
    "alpha0,alpha=svm_fit(Xsub,label_sub,Nbatch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "svm_pred=svm_predict(Xsub,Xsub,alpha,8)\n",
    "check_predictions(svm_pred,label_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# SVM Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Since apparently the training time for a SVM goes as $O(n_{sample}^3)$, maybe it is better to train an ensemble of SVMs.\n",
    "In which case the training time is $O(n_{sample}^3/n_{ensemble^2})$ for the ensemble.  Then evaluating the results typically takes $O(n_sample)$ for all of the ensemble together.  (This is something like making the crude assumption that the kernels are block-diagonal, once appropriately sorted).  If we repeat this for multiple such random splits we can extract different correlations.\n",
    "We Then take a majority vote.\n",
    "\n",
    "A similar idea is available here:(https://stackoverflow.com/questions/31681373/making-svm-run-faster-in-python), which suggests\n",
    "using a BaggingClassifier to automate the process.  \n",
    "Of course, Random Forests are another option, with a similar goal.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "nfeature,nobs=X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#just use bagging classifier on the whole list of SVMs\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#make the SVM model\n",
    "svm=SVC(cache_size=200,gamma=0.1,C=0.5,class_weight='balanced')\n",
    "#The bagging classifier of those\n",
    "ensemble_svm=BaggingClassifier(svm,n_estimators=20,\n",
    "bootstrap=False,n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Try to determine parameters gamma/C via cross-validation.\n",
    "#Note that there is no need for explicit regularization?  Apparently in large dimensions, the parameters C/gamma (for penalty radius and width of basis function do a decent job in regularizing), since l1, l2 regularization don't work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-1ba2253447d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindsub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXsub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_sub\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac_perc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train_counts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'toxic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#use the ravel for reshaping?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mensemble_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXsub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msvm_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensemble_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXsub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#test on a different subset of the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0mtotal_n_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 verbose=self.verbose)\n\u001b[0;32m--> 375\u001b[0;31m             for i in range(n_jobs))\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frac_perc=0.1\n",
    "t0=time.time()\n",
    "#svm=SVC(cache_size=1000,verbose=True,gamma=0.1,C=0.5,class_weight='balanced')\n",
    "indsub,Xsub,label_sub=get_subset(frac_perc,X_train_counts,df_train['toxic'].values)\n",
    "#use the ravel for reshaping?\n",
    "ensemble_svm.fit(Xsub,label_sub.ravel())\n",
    "svm_pred=ensemble_svm.predict(Xsub)\n",
    "#test on a different subset of the training data\n",
    "frac_perc2=0.01\n",
    "indsub2,Xsub2,label_sub2=get_subset(frac_perc2,X_train_counts,df_train['toxic'].values)\n",
    "svm_pred2=ensemble_svm.predict(Xsub2)\n",
    "t1=time.time()\n",
    "print('Time Elapsed:',t1-t0)\n",
    "svm_stats=check_predictions(svm_pred,label_sub)\n",
    "svm_stats2=check_predictions(svm_pred2,label_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Randomized Fourier Features\n",
    "\n",
    "The Tensorflow documentation includes a great idea for extending Kernel machines: use an sinusoidal mapping from the original space to another linear space.  The mapping depends on a Gaussian random variable, so when we take expectation values over the Gaussian variable, the result\n",
    "of that expectation approximates the desired kernel.  Genius!\n",
    "Ideas here:(https://www.tensorflow.org/tutorials/kernel_methods,\n",
    "https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf).\n",
    "See also scikit-learn's Kernel Approximations methods, which implement the RBF kernel described below. \n",
    "\n",
    "LinearSVMs work quickly, but their full kernel counterparts are slow to train, scaling as $O(n_{sample}^3)$.\n",
    "Instead, consider features like \n",
    "\\begin{equation}\n",
    "    z_{k}(\\mathbf{x})=\\cos(\\mathbf{\\omega}_{k}\\cdot\\mathbf{x}+b_{k}),\n",
    "\\end{equation}\n",
    "where $\\mathbf{x}\\in \\mathbb{R}^{d}, \\omega\\in \\mathbb{R}^{d},\\mathbf{b}_{k}\\in\\mathbb{R}$, and $\\omega_{k}$, is a random Gaussian vector drawn from\n",
    "\\begin{equation}\n",
    "    P(\\omega) = (2\\pi\\sigma^2)^{-d/2} \\exp\\left(-\\frac{\\mathbf{\\omega}^2}{2\\sigma^2}\\right),\n",
    "\\end{equation}\n",
    "and $b_{k}$ is a uniform random variable drawn from $[0,2\\pi)$.  Note that $z_{k}$ is a scalar.  But if we consider making $D$ draws of the random variables, then we can construct a vector $\\mathbf{z}(\\mathbf{x})=\\sqrt{\\frac{2}{D}}[z_{1},z_{2},\\ldots, z_{D}]$,\n",
    "\n",
    "The inner products on these new feature vectors for different input data are given y \n",
    "\\begin{equation}\n",
    "    \\mathbf{z}(\\mathbf{x})\\cdot\\mathbf{z}(\\mathbf{y})=\\frac{2}{D}\\sum_{k=1}^{D} \\cos(\\mathbf{\\omega}_{k}\\cdot\\mathbf{x}+b_{k})\\cos(\\mathbf{\\omega}_{k}\\cdot\\mathbf{y}+b_{k}).\n",
    "\\end{equation}\n",
    "This is essentially a Monte-Carlo estimate (with $D$ samples) of the probability distributions.  As $D\\rightarrow \\infty$, this converges to \n",
    "\\begin{align}\n",
    "    \\mathbf{z}(\\mathbf{x})\\cdot\\mathbf{z}(\\mathbf{y})&\\approx \\int d\\mathbf{\\omega}\\int db\\,P(\\omega)p(b)\n",
    "    2\\cos(\\mathbf{\\omega}\\cdot\\mathbf{x}+b)\\cos(\\mathbf{\\omega}\\cdot\\mathbf{y}+b)\\\\\n",
    "&=\\frac{1}{2\\pi}\\frac{1}{(2\\pi \\sigma^2)^{D/2}}\\int d\\mathbf{\\omega}\\int_0^{2\\pi} db\\,e^{-(\\mathbf{\\omega})^2/(2\\sigma^2)}\n",
    "    2\\cos(\\mathbf{\\omega}\\cdot\\mathbf{x}+b)\\cos(\\mathbf{\\omega}\\cdot\\mathbf{y}+b) \\\\\n",
    "&=\\frac{1}{2\\pi}\\frac{1}{(2\\pi \\sigma^2)^{D/2}}\\int d\\mathbf{\\omega}\\int_0^{2\\pi} db\\,e^{-(\\mathbf{\\omega})^2/(2\\sigma^2)}\n",
    "    \\bigg(\\cos[\\mathbf{\\omega}\\cdot(\\mathbf{x}+\\mathbf{y})+b]+\\cos[\\mathbf{\\omega}\\cdot(\\mathbf{x}-\\mathbf{y})]\\bigg),\n",
    "\\end{align}\n",
    "where we used a double-angle formula on the cosines.  The Gaussian and uniform integrals can be carried out, with the result\n",
    "\\begin{align}\n",
    "    \\mathbf{z}(\\mathbf{x})\\cdot\\mathbf{z}(\\mathbf{y})&\\approx \n",
    "&=\\,e^{-(\\mathbf{x-y})^2/(2\\sigma^2)}.\n",
    "\\end{align}\n",
    "The same idea can be extended for any $P(\\mathbf{\\omega})$ to get the desired kernel, provided it has a nice Fourier transform.\n",
    "\n",
    "One thing noted in the docs is that this works well for smooth data, but can require a lot of components if there is a significant random component, such as trying to detect fractal structures like forests.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Deep Network\n",
    "\n",
    "Another idea is to build a deep neural network on the term-frequency matrix, effectively running with extensions to the Naive Bayes model.\n",
    "This will use the reduced term-frequency matrix after the Truncated SVD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, l2_regularizer\n",
    "from tensorflow.contrib.rnn import BasicRNNCell,LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from deep_network import deep_dropout_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb37cb24a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter #5000. Current log-loss:0.06816151738166809\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing regularization_losses.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'function' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "actual=df_train['toxic'].astype(int).values\n",
    "save_name='tf_models/deep_relu_drop'\n",
    "dNN=deep_dropout_NN(X_train_trans.shape)\n",
    "dNN.run_graph(X_train_trans,actual,save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_models/deep_relu_drop-5000\n"
     ]
    }
   ],
   "source": [
    "#model_name='tf_models/deep_relu_drop-{}'.format(dNN.n_iter)\n",
    "model_name='tf_models/deep_relu_drop-{}'.format(5000)\n",
    "dnn_pred=dNN.predict_all(model_name,X_train_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95554, 1) (95554, 1)\n",
      "True Positive 0.07975594951545723. False Positive 0.01246415639324361\n",
      "False Negative 0.016932833790317518. True Negative 0.8908470603009816\n",
      "Log-loss is 1.0153460369408245\n",
      "AUROC is 0.9055372623991556\n"
     ]
    }
   ],
   "source": [
    "dnn_conf=check_predictions(dnn_pred,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95554, 1) (95554, 1)\n",
      "True Positive 0.08529208615023966. False Positive 0.016461895891328463\n",
      "False Negative 0.01139669715553509. True Negative 0.8868493208028968\n",
      "Log-loss is 0.9622148788120853\n",
      "AUROC is 0.931953076744998\n"
     ]
    }
   ],
   "source": [
    "nb_conf=check_predictions(pred_nb,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Predictions from Deep Neural Network\n",
    "\n",
    "Let's now run some predictions on the full training and development sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-3cf7923568e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_models/deep_relu_drop-{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnn_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'3 layer ReLU network'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-85db29b64535>\u001b[0m in \u001b[0;36mnetwork_predict\u001b[0;34m(model_name, input_data)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mNobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mnn_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mnn_pred_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mi0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_models/deep_relu_drop-10000\n"
     ]
    }
   ],
   "source": [
    "model_name='tf_models/deep_relu_drop-{}'.format(n_iter)\n",
    "\n",
    "nn_pred_train = network_predict(model_name,X_train_trans)\n",
    "\n",
    "print('3 layer ReLU network')\n",
    "check_predictions(nn_pred_train,actual)\n",
    "print('Naive Bayes')\n",
    "check_predictions(pred_nb,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer ReLU network: on Dev set\n",
      "(32083, 1) (32083, 1)\n",
      "True Positive 0.05794345915282237. False Positive 0.023096343858118006\n",
      "False Negative 0.037652339245083065. True Negative 0.8813078577439766\n",
      "Log-loss is 2.0982036497639474\n",
      "AUROC is 0.7902960670474108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_models/deep_relu_drop-5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer ReLU network: on Dev set\n",
      "(95554, 1) (95554, 1)\n",
      "True Positive 0.08278041735563137. False Positive 0.007043137911547397\n",
      "False Negative 0.013908365950143374. True Negative 0.8962680787826779\n",
      "Log-loss is 0.7236449386910208\n",
      "AUROC is 0.9241781204032228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_models/deep_relu_drop-5000\n"
     ]
    }
   ],
   "source": [
    "#Try testing on the dev-set\n",
    "model_name='tf_models/deep_relu_drop-{}'.format(dNN.n_iter)\n",
    "nn_pred_train = dNN.predict_all(model_name,X_train_trans)\n",
    "print('4 layer ReLU network: on Dev set')\n",
    "actual_train=df_train['toxic'].values\n",
    "nn_stats=check_predictions(nn_pred_train,actual_train)\n",
    "\n",
    "nn_pred_dev = dNN.predict_all(model_name,X_dev_trans)\n",
    "print('4 layer ReLU network: on Dev set')\n",
    "actual_dev=df_dev['toxic'].values\n",
    "nn_stats=check_predictions(nn_pred_dev,actual_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes on Dev set\n",
      "(32083, 1) (32083, 1)\n",
      "True Positive 0.06299286226350403. False Positive 0.023501542873172708\n",
      "False Negative 0.0326029361344014. True Negative 0.8809026587289218\n",
      "Log-loss is 1.9377988469688499\n",
      "AUROC is 0.8164822255177966\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes on Dev set')\n",
    "pred_dev_nb=nb.predict(X_dev_counts)\n",
    "nb_stats=check_predictions(pred_dev_nb,actual_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Evidently the (3-layer ReLU-tanh) network is over-fitting to the training set.  Not surprising, since there is no regularization here.\n",
    "It could outperform the Naive Bayes method on the training set, but had worse performance on the development dataset.\n",
    "\n",
    "Let's put in some dropout. Putting in dropout after each layer, with a 0.1 dropout probability improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.63848495096381463, 0.69363963655066008]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dev scores\n",
    "[f1_score(actual_dev,nn_pred_dev),f1_score(actual_dev,pred_dev_nb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94606310013717421, 0.85717301805130364]\n",
      "[0.63848495096381463, 0.69363963655066008]\n"
     ]
    }
   ],
   "source": [
    "print([f1_score(actual,nn_pred_train),f1_score(actual,pred_nb)])\n",
    "print([f1_score(actual_dev,nn_pred_dev),f1_score(actual_dev,pred_dev_nb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True], dtype=bool)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.round([1.0, 0.0, 0.1, 0.9])).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(956, 1)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "I am finding that beyond one or two layers, the network just seems to output zeros.  Maybe the learning rate was too high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `sklearn` not found.\n"
     ]
    }
   ],
   "source": [
    "?sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb754de748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checks output for a single training batch.\n",
    "plt.figure()\n",
    "plt.hist(nn_pred[y_batch[:,0],0],bins=20)\n",
    "plt.hist(nn_pred[~y_batch[:,0],0],bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb75311e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(nn_pred_total,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "So let's try the current flavour of the month approach: a recurrent neural network.\n",
    "Based on talking to Joseph and Fahim at the group, they used a two-layer neural network based on the just the 2000 most common words, using ReLU activation.(I think they said their approach was inspired by someone at Kaggle.)\n",
    "Let's try something similar, with initially a single layer leaky ReLU layer, but after using a Truncated SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "init_explore.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
