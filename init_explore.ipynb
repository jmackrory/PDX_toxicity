{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Toxicity in Wikipedia Comments\n",
    "\n",
    "This is a parallel work to any work on the wikipedia toxicity data on the same\n",
    "topic.  This data has not been cleaned yet, and has not had multiple categories introduced yet.  However it is presented free from bias, for people to play with.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3582, 7) (35152, 4)\n"
     ]
    }
   ],
   "source": [
    "df_com = pd.read_csv('data/toxicity_annotated_comments_unanimous.tsv',sep='\\t')\n",
    "df_rate = pd.read_csv('data/toxicity_annotations_unanimous.tsv',sep='\\t')\n",
    "#df_com = pd.read_csv('data/toxicity_annotated_comments.tsv',sep='\\t')\n",
    "#df_rate = pd.read_csv('data/toxicity_annotations.tsv',sep='\\t')\n",
    "\n",
    "#make rev_id an integer\n",
    "df_com['rev_id']=df_com['rev_id'].astype(int)\n",
    "df_rate['rev_id']=df_rate['rev_id'].astype(int)\n",
    "\n",
    "#reindex \n",
    "#df_com.index=df_com['rev_id']\n",
    "#df_rate.index=df_rate['rev_id']\n",
    "print(df_com.shape, df_rate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#make a new column in df_com with array of worker_ids, and toxicity\n",
    "df_com['scores']=None\n",
    "\n",
    "#since 'rev_id' is sorted, can take first difference, and find where\n",
    "#there are changes.  Those set the boundaries for changes.\n",
    "#Need to also append final index.\n",
    "change_indices=df_rate.index[df_rate['rev_id'].diff()!=0].values\n",
    "change_indices=np.append(change_indices,len(df_rate))\n",
    "\n",
    "for i in range(len(change_indices)-1):\n",
    "    ind0 = change_indices[i]\n",
    "    ind1 = change_indices[i+1]\n",
    "    d0=df_rate.iloc[ind0:ind1]\n",
    "    scores=d0[['worker_id','toxicity_score']]\n",
    "    #pass it a list so it can be set as an entry.\n",
    "    #accessing later will require score[0] idiocy to get at list.\n",
    "    df_com.loc[i,'scores']=[scores.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "?np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_com['mean_toxic']=df_com['scores'].apply(score_mean)\n",
    "df_com['toxic']=df_com['mean_toxic']!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def score_mean(score_list):\n",
    "    \"\"\"score_mean\n",
    "    Compute mean of toxicity scores for input array.\n",
    "    Array is first (and only) element in the input list.\n",
    "    Compute mean running down the rows.  Could be updated to include weighted sum of weights\n",
    "    \"\"\"\n",
    "    s_arr=score_list[0]\n",
    "    s = np.mean(s_arr[:,1])\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_com['toxic'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#can combine the dataframes together by extracting all reviewer ids, and scores for each "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Looking at the comments data, we'll need to clean the data quite a bit (lots of newlines, weird characters).\n",
    "\n",
    "My rough plan is to build up a lexicon, tokenize that data, and try to build a Naive Bayes model.  (Maybe later a Recurrent Neural network model?)\n",
    "\n",
    "Other Analysis possibilities:\n",
    "* Support Vector Machine\n",
    "    - the other big architecture, less popular now?\n",
    "* Recurrent Neural Network\n",
    "    - Build up word embeddings (word2vec), or just use the pretrained ones.\n",
    "* Naive Bayes\n",
    "    - can find most important words in spam\n",
    "    - simple, easy to understand baseline.\n",
    "* Latent Factor Analysis \n",
    "    - maybe useful prelude for building up embeddings.\n",
    "    \n",
    "Cleaning:\n",
    "* Tokenize (convert words to indices)\n",
    "* Clean data : How to remove newlines (search/replace: NEWLINE with '')\n",
    "* Stemming words\n",
    "* Balancing data set\n",
    "* Match up comments, and review scores\n",
    "\n",
    "* Search for gibberish words (make a new \"feature\" for badly spelled comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#cleaning the data\n",
    "#Can use pandas built in str functionality with regex to eliminate\n",
    "\n",
    "#maybe also dates?\n",
    "def clean_up(comments):\n",
    "    com_clean=comments.str.replace('NEWLINE_TOKEN',' ')\n",
    "    com_clean=com_clean.str.replace('TAB_TOKEN',' ')    \n",
    "    #Remove HTML trash, via non-greedy replacing anything between backticks\n",
    "    com_clean=com_clean.str.replace(\"style=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"class=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"width=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"align=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"cellpadding=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"cellspacing=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"rowspan=\\`\\`.*?\\`\\`\",' ')\n",
    "    com_clean=com_clean.str.replace(\"colspan=\\`\\`.*?\\`\\`\",' ')\n",
    "    #remove numbers\n",
    "    com_clean=com_clean.str.replace(\"[0-9]+\",' ')\n",
    "    #remove symbols.    \n",
    "    com_clean=com_clean.str.replace(\"\\[\\[\",' ')\n",
    "    com_clean=com_clean.str.replace(\"\\]\\]\",' ')\n",
    "    com_clean=com_clean.str.replace(\"==\",' ')\n",
    "    com_clean=com_clean.str.replace(\"|\",' ')                                \n",
    "    \n",
    "    #remove multiple spaces, replace with a single space\n",
    "    com_clean=com_clean.str.replace('\\\\s+',' ')\n",
    "\n",
    "    #remove symbols\n",
    "    return com_clean\n",
    "df_com['comment_clean']=clean_up(df_com['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159686, 172992)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#borrowing from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "count_vect=CountVectorizer(stop_words='english',lowercase=True,strip_accents='unicode')\n",
    "X_train_counts=count_vect.fit_transform(df_com['comment_clean'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cond_prob(X_counts,toxic):\n",
    "    \"\"\"bayes_prob\n",
    "    X_counts - sparse matrix of counts of each word in a given message\n",
    "    toxic - whether word was toxic or not, with 0,1\n",
    "    \"\"\"\n",
    "    ptoxic = np.sum(toxic)/len(toxic)\n",
    "\n",
    "    toxic_mat=X_counts[toxic==1,:]\n",
    "    clean_mat=X_counts[toxic!=1,:]\n",
    "    #sum across messages\n",
    "    nword_toxic=np.sum(toxic_mat,axis=0)\n",
    "    nword_clean=np.sum(clean_mat,axis=0)    \n",
    "    nwords=len(nword_clean)\n",
    "    #estimate probability of word given toxicity by number of times\n",
    "    #that word occurs in toxic documents, divided by the total number of words\n",
    "    #in toxic documents\n",
    "    #Laplace smooth version\n",
    "    #pword_toxic=(nword_toxic+1)/np.sum(nword_toxic+nwords)\n",
    "    #pword_clean=(nword_clean+1)/np.sum(nword_clean+nwords)\n",
    "    #raw version    \n",
    "    pword_toxic=(nword_toxic)/np.sum(nword_toxic)\n",
    "    pword_clean=(nword_clean)/np.sum(nword_clean)    \n",
    "\n",
    "    return ptoxic,pword_toxic,pword_clean    \n",
    "\n",
    "ptox,pw_tox,pw_cln = cond_prob(X_train_counts,df_com['toxic'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#get vocabulary dictionary\n",
    "voc_dict=count_vect.vocabulary_\n",
    "#make a dataframe, with entries as rows\n",
    "voc_df=pd.DataFrame.from_dict(voc_dict,orient='index')\n",
    "#sort by row entry value\n",
    "voc_df1=voc_df.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "xtot=X_train_counts.sum(axis=0).squeeze()\n",
    "#compare vectorized vs. naive counts to check mappings\n",
    "def check_vect(count_mat,comments,vocab,word):\n",
    "\n",
    "    ind=vocab.loc[word].values\n",
    "    xtot=count_mat.sum(axis=0)\n",
    "    vect_count=(xtot[0,ind])\n",
    "    com_count=np.sum(comments.str.contains('{}'.format(word)))\n",
    "    msk=(count_mat[:,ind]>0).toarray().squeeze()\n",
    "    comments=comments[msk]\n",
    "    return vect_count,com_count,comments\n",
    "\n",
    "vc,cc,com=check_vect(X_train_counts,df_com['comment_clean'],voc_df,'fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vect: [[14]], Com: 17\n",
      "97       Excuse me, whoever the Fuck you are SPUI says on his talk page that he likes vandalism, and doe...\n",
      "339                                                                             I got your message Fuck you\n",
      "729      FUCK YOU YOU FUCKING FAGGOT!!!! this is Sean MC Sean. You motherfuckers blocked me for a long time\n",
      "939                                 What... What are you doing? Don't fuck me. I fuck to Ryulong, not you. \n",
      "972      FUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OF...\n",
      "1310                Hey go fuck yourself. Why don't you go edit my talk page to show how offended you are. \n",
      "1409                                     What the fuck are you talking about? They did lose a couple times.\n",
      "1520     Fuck You Go suck on a fat dick faggot mother fucker. I'm the the wrong person to fuck with you ...\n",
      "2000                                                           wtf? this aint me! who the fuck used ma ip?!\n",
      "2040             You Cunt Go fuck yourself you piece of shit. Don't make me tea beg your mama you faggot!!!\n",
      "2313     Go fuck yourselves, obviously you all are a bunch of pricks who claim to be admins but don't do...\n",
      "2601    `::::::: said to me ``shit-stirring troll: Fuck off, moron.`` on WP:ANI, Is that also a breach o...\n",
      "Name: comment_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('Vect: {}, Com: {}'.format(vc,cc))\n",
    "print(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3279])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word_mat=np.array([X_train_counts.sum(axis=0),pw_cln,pw_tox]).squeeze()\n",
    "word_df=pd.DataFrame(word_mat.T,columns=['count','p_clean','p_toxic'],index=voc_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      2.000000\n",
       "p_clean    0.000041\n",
       "p_toxic    0.000077\n",
       "Name: dick, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.loc['dick']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      count   p_clean   p_toxic           word\n",
       "0       1.0  0.000041  0.000000           like\n",
       "5458    2.0  0.000083  0.000000         carpio\n",
       "5457    2.0  0.000083  0.000000           belz\n",
       "5456    1.0  0.000041  0.000000       applying\n",
       "5455    6.0  0.000248  0.000000          durra\n",
       "5454    1.0  0.000041  0.000000      cornelius\n",
       "5452    1.0  0.000041  0.000000       partners\n",
       "5451    1.0  0.000041  0.000000        stories\n",
       "5448    3.0  0.000124  0.000000        peoples\n",
       "5447    1.0  0.000041  0.000000      critisize\n",
       "5446    1.0  0.000041  0.000000        oldlady\n",
       "5445    2.0  0.000083  0.000000         jejune\n",
       "5444    1.0  0.000041  0.000000        rethink\n",
       "5443    1.0  0.000041  0.000000            mau\n",
       "5437    3.0  0.000124  0.000000        demonic\n",
       "5435    1.0  0.000041  0.000000             ft\n",
       "5434    1.0  0.000041  0.000000        biggest\n",
       "5433    1.0  0.000041  0.000000      establish\n",
       "5432    1.0  0.000041  0.000000        wanting\n",
       "5430    1.0  0.000041  0.000000      discribes\n",
       "5429    2.0  0.000083  0.000000      setuation\n",
       "5428    1.0  0.000041  0.000000         suites\n",
       "5427    2.0  0.000083  0.000000      ignorance\n",
       "5426    1.0  0.000041  0.000000           trem\n",
       "5459    1.0  0.000041  0.000000        esteban\n",
       "5460    2.0  0.000083  0.000000        scratch\n",
       "5461    1.0  0.000041  0.000000         design\n",
       "5463    1.0  0.000041  0.000000         fridge\n",
       "5490    1.0  0.000041  0.000000       talkback\n",
       "5489    2.0  0.000083  0.000000           tarc\n",
       "...     ...       ...       ...            ...\n",
       "9068  122.0  0.002394  0.004951       constate\n",
       "2581   67.0  0.000000  0.005183           shit\n",
       "1592   67.0  0.000000  0.005183        midgley\n",
       "2007   69.0  0.000000  0.005338           base\n",
       "5619   71.0  0.000000  0.005492         louisa\n",
       "775    72.0  0.000000  0.005570        eduardo\n",
       "8244   79.0  0.000248  0.005647          robes\n",
       "8715  110.0  0.001445  0.005802  comparatively\n",
       "7465   78.0  0.000000  0.006034          glory\n",
       "4291   78.0  0.000000  0.006034     seljukians\n",
       "940    78.0  0.000000  0.006034          marco\n",
       "135    91.0  0.000454  0.006189           hear\n",
       "7979   81.0  0.000000  0.006266          dones\n",
       "3002   82.0  0.000041  0.006266       hartzell\n",
       "2304   82.0  0.000000  0.006343      according\n",
       "4787  124.0  0.001692  0.006421          error\n",
       "6690   92.0  0.000372  0.006421         circle\n",
       "3978  137.0  0.002023  0.006807      dartmouth\n",
       "1591   98.0  0.000000  0.007581     procedures\n",
       "5401   98.0  0.000000  0.007581      antitrust\n",
       "4897  100.0  0.000000  0.007736           ludo\n",
       "3274  100.0  0.000000  0.007736     discovered\n",
       "8246  117.0  0.000289  0.008509  jessielturner\n",
       "509   283.0  0.006151  0.010366        instead\n",
       "9082  703.0  0.023364  0.010598            lui\n",
       "5961  399.0  0.008793  0.014388       doggzzzz\n",
       "1590  222.0  0.000000  0.017173        portion\n",
       "3277  223.0  0.000000  0.017251         prhchi\n",
       "7978  231.0  0.000000  0.017870     especially\n",
       "5776  360.0  0.000000  0.027849       clang_zh\n",
       "\n",
       "[9351 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.sort_values('p_toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Looks like my word mapping is completely screwed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9351)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes(mat,pword_tox,pword_cln,ptox):\n",
    "    \"\"\"Compute probability that a message \n",
    "    is toxic via naive_bayes estimate.\n",
    "    \"\"\"\n",
    "    msk = mat>0\n",
    "    #ptox_word = (pword_tox * p_tox)/(pword_cln*pcln + pword_tox * p_tox)\n",
    "    #Need to multiply together.  Can just add the logs, and exponentiate at\n",
    "    #the end\n",
    "\n",
    "    #do once for all words\n",
    "    log_pword = np.log(pword_tox*ptox/(pword_tox*ptox + pword_cln*(1-ptox)))\n",
    "    log_score = mat.dot(log_pword.T)\n",
    "    score=np.exp(log_score)\n",
    "    return score\n",
    "scores=naive_bayes(X_train_counts,pw_tox,pw_cln,ptox)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efba3f0e358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                            comment_clean  \\\n",
       "18                                                                                       :Are you there?    \n",
       "22                                                                                               * Keep.    \n",
       "25                                                                                               * Keep.    \n",
       "73                                                                                                          \n",
       "82                                                                                                     —    \n",
       "86                                                                                                ` : : `   \n",
       "139                                                                                      ::He always is.    \n",
       "159                                                                                 What else is on her?    \n",
       "165                                                                                                    /    \n",
       "166                                                                              :What are you on about?    \n",
       "222                                                                                                         \n",
       "229                                                                                                   ` `   \n",
       "318                                                                               And beyond all this, it   \n",
       "350                                                                              (among others) were made   \n",
       "371                                                                                              * - May    \n",
       "441                                                                               :or is it Here, here?.    \n",
       "454                                                         these idiots............ Mitchell Starrenburg   \n",
       "590                                                                                 This is one of them.    \n",
       "771                                                                                                  : ,    \n",
       "778                                        YOU SUCK YOU SUCK YOU SUCK YOU SUCK YOU SUCK YOU SUCK YOU SUCK   \n",
       "843                                                                                       Another one: .    \n",
       "898                                                                    ` :Not the only one either! '''' `   \n",
       "914                                                                        many, many, many, many, many}}   \n",
       "936                                                                          What is it and where is it?    \n",
       "972    FUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OFFFUCK OF...   \n",
       "1165                                                                                      :It is here. -    \n",
       "1231                                                                                              ` May `   \n",
       "1266                                                                              :It's not even there??    \n",
       "1283                                                                                                  ` `   \n",
       "1359                                                                                            :Why so?    \n",
       "...                                                                                                   ...   \n",
       "2026                                                                              ` ::::: how about no. `   \n",
       "2062                                                                                     ` ''''' : , May`   \n",
       "2095                                                                         I am still a gigantic faggot   \n",
       "2098                                                                                         Some should    \n",
       "2174                                                                              ` And so what's this? `   \n",
       "2197                                                                                           : Me too!    \n",
       "2256                                                                                            ::done ·    \n",
       "2275                                                                            ` Another one. '''''' - `   \n",
       "2330                                                                             (After I found out how)    \n",
       "2374                                                                        ::::::This one and this one.    \n",
       "2402                                                                   : I had enough of *you*, bastard.    \n",
       "2426                                                                               do so sexually with me   \n",
       "2459                                                                       ::Been there, done that. -) -    \n",
       "2478                                                                             Bongwarrior is a faggot.   \n",
       "2480                                                                      What what what what is this ???   \n",
       "2591                                                                                         . . . . . .    \n",
       "2618                                                                                         ` : '''''' `   \n",
       "2634                                                                                      And one at . -    \n",
       "2662                                                                                                   ~    \n",
       "2759                                                                                             . . . •    \n",
       "2864                                                                       ` Why Why are you so stupid? `   \n",
       "2958                                                                 Then you're both blind and retarded.   \n",
       "2984                                                                                        ` ::Done! ☆ `   \n",
       "3133                                                                               , as will as the third   \n",
       "3163                                                                                     is , , and could   \n",
       "3206                                                                                                   /    \n",
       "3215                                                                  , how about one of these for you??    \n",
       "3224                                                                             ::Or, perhaps: ::: :: —    \n",
       "3234                                                                                            :See . —    \n",
       "3328                                                                                           ::You to.    \n",
       "\n",
       "      toxic  \n",
       "18    False  \n",
       "22    False  \n",
       "25    False  \n",
       "73    False  \n",
       "82    False  \n",
       "86    False  \n",
       "139   False  \n",
       "159   False  \n",
       "165   False  \n",
       "166   False  \n",
       "222   False  \n",
       "229   False  \n",
       "318   False  \n",
       "350   False  \n",
       "371   False  \n",
       "441   False  \n",
       "454    True  \n",
       "590   False  \n",
       "771   False  \n",
       "778    True  \n",
       "843   False  \n",
       "898   False  \n",
       "914   False  \n",
       "936   False  \n",
       "972    True  \n",
       "1165  False  \n",
       "1231  False  \n",
       "1266  False  \n",
       "1283  False  \n",
       "1359  False  \n",
       "...     ...  \n",
       "2026  False  \n",
       "2062  False  \n",
       "2095   True  \n",
       "2098  False  \n",
       "2174  False  \n",
       "2197  False  \n",
       "2256  False  \n",
       "2275  False  \n",
       "2330  False  \n",
       "2374  False  \n",
       "2402   True  \n",
       "2426   True  \n",
       "2459  False  \n",
       "2478   True  \n",
       "2480  False  \n",
       "2591  False  \n",
       "2618  False  \n",
       "2634  False  \n",
       "2662  False  \n",
       "2759  False  \n",
       "2864   True  \n",
       "2958   True  \n",
       "2984  False  \n",
       "3133  False  \n",
       "3163  False  \n",
       "3206  False  \n",
       "3215  False  \n",
       "3224  False  \n",
       "3234  False  \n",
       "3328  False  \n",
       "\n",
       "[73 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the messages classified as spam.\n",
    "msk = np.array(scores>0.5)\n",
    "df_com[msk][['comment_clean','toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4887332794685343e-05"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(pw_tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   2.00000000e+00,   1.00000000e+00, ...,   1.00000000e+00,   1.00000000e+00,   1.00000000e+00],\n",
       "       [  5.95663569e-05,   8.93495354e-05,   5.95663569e-05, ...,   5.95663569e-05,   5.95663569e-05,   5.95663569e-05],\n",
       "       [  4.48873328e-05,   4.48873328e-05,   4.48873328e-05, ...,   4.48873328e-05,   4.48873328e-05,   4.48873328e-05]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_mat.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test=['Zep the blah blah','Cow Alt the blah Alt','foo the if']\n",
    "test2=count_vect.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt': 0, 'blah': 1, 'cow': 2, 'foo': 3, 'zep': 4}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 0, 1],\n",
       "       [2, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.sum(axis=0).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#get term frequencies via tfidf transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So, I briefly embarassed myself by looking for racist slurs,\n",
    "and found none of the obvious American candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df_com['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck 57401\n",
      "shit 136209\n",
      "cunt 35357\n",
      "piss 114922\n",
      "cocksucker 29180\n",
      "dick 40351\n",
      "ass 10569\n",
      "asshole 10688\n",
      "bitch 17516\n"
     ]
    }
   ],
   "source": [
    "naughty_word=['fuck','shit','cunt','piss','cocksucker','dick','ass','asshole','']\n",
    "for word in naughty_word:\n",
    "    try:\n",
    "        print(word,count_vect.vocabulary_[word])\n",
    "    except:\n",
    "        print(word,'not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "init_explore.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
